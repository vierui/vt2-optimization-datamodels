Below is a high-level proposal for a step-by-step implementation plan. Because there are a few moving parts—particularly around structuring the code, scaling/weighting each representative week, handling storage states, and automating the pipeline—I’ll walk you through the essential design decisions and tasks. Once you confirm each step, we can refine further or provide sample code snippets.

⸻

1. Confirm Core Assumptions
	1.	Seasonal Weeks
	•	You’ve specifically chosen Week 2 for winter, Week 31 for summer, and Week 43 for spring/autumn.
	•	Each of these weeks has 168 consecutive hourly data points (7 days * 24 hours).
	2.	Weighting/Multiplying Factor
	•	Winter (Week 2) is repeated 13× to represent ~13 winter weeks.
	•	Summer (Week 31) is repeated 13× to represent ~13 summer weeks.
	•	Spring/Autumn (Week 43) is repeated 26× to represent ~26 spring/autumn weeks.
	3.	Storage State Initialization
	•	Each representative block (winter, summer, spring/autumn) starts with a “neutral” SoC assumption (e.g., 50%), ignoring transitions from the previous block. This simplifies multi-season transitions.
	4.	Hourly Resolution
	•	All data (load, wind, solar) is hourly.
	5.	Available Generators & Storage Assets
	•	You will define or “declare” all possible generators and storage units in some form of metadata (e.g., a CSV or a Python dictionary) so you can easily scale them or turn them on/off in the model.
	•	The model then references these declared assets for each time-step in the final aggregated time series.
	6.	Automated Pipeline
	•	We will create a workflow that does the following automatically:
	1.	Read raw CSV data for an entire year (load, wind, solar).
	2.	Extract the specific weeks (2, 31, 43).
	3.	Resample or directly keep them at hourly resolution if already hourly.
	4.	Replicate/weight each week to reflect its share of the year.
	5.	Construct a final “time series” table of 52 weeks (13 + 13 + 26).
	6.	Assign the data to the correct bus/generator ID.
	7.	Run the DCOPF (or investment DCOPF).
	8.	Multiply or incorporate weighting factors in the objective function.
	9.	Report results in a single run that approximates the entire year.

Once these assumptions are good to go, we can implement the solution.

⸻

2. Data Preparation & Parsing
	1.	Load Data
	•	You likely have CSVs of load, wind generation, solar generation for the full year (8760 hours).
	•	Example:

time,value
2023-01-01 00:00:00,36.8
2023-01-01 01:00:00,26.5
...


	•	Read them into Pandas DataFrames, ensuring the time column is parsed as datetime.

	2.	Extract Weeks 2, 31, 43
	•	Identify the exact datetime ranges:
	•	Week 2 is typically Jan 8–14 if using an ISO standard or Jan 9–15 if using another offset. So be sure to define precisely how you pick “week 2.”
	•	Similarly for weeks 31 and 43.
	•	For example, a robust approach is:

# Pseudocode
winter_week_data = full_year_df.loc["2023-01-08 00:00":"2023-01-14 23:00"]
summer_week_data = full_year_df.loc["2023-07-30 00:00":"2023-08-05 23:00"]
spring_autumn_week_data = full_year_df.loc["2023-10-22 00:00":"2023-10-28 23:00"]


	•	Confirm you’re slicing exactly 168 hours each time.

	3.	Check for Gaps
	•	Make sure each slice is 168 hours. If there are missing data points, you might need to fill or interpolate.
	4.	Resample
	•	If your data is already hourly, you probably don’t need to do anything. If not, you can do df.resample('H').mean() or something similar.

⸻

3. Construct the Representative Time Series

Single “Block” Per Season with Weighting in the Objective (option B)
	•	You keep only 3 blocks of 168 hours but use an objective weight factor for each block.
	•	This means your final time series is only 3 × 168 = 504 hours.
	•	Then you instruct the solver that the cost (or the generation cost variables) for the winter block should be multiplied by 13, the summer block by 13, and the spring/autumn block by 26.
	•	Pros:
	•	The model is much smaller and faster to solve.
	•	Cons:
	•	You must carefully handle the cost weighting for each block.
	•	You must handle or ignore any cross-block constraints (storage SoC, etc.).


Implementing Option in Practice:
	1.	Create a “mega timeseries” with 3 disconnected segments:
	•	Hours 0–167: Winter
	•	Hours 168–335: Summer
	•	Hours 336–503: Spring/Autumn
	2.	Create a mapping for each hour to its “block weight”.
	•	For winter block hours: block weight = 13.
	•	For summer block hours: block weight = 13.
	•	For spring/autumn block hours: block weight = 26.
	3.	In your DCOPF code, you have a cost coefficient for each generator at each hour. You can multiply that coefficient by the block weight. (This is an easy place to incorporate weighting: cost[t] = original_cost * block_weights[t].)

This effectively says “the cost in this hour is repeated for X weeks,” so the optimizer’s objective sees that block’s generation cost as X times more expensive or beneficial.

⸻

4. Handling Storage Reset Between Blocks

If you’re using Option B (3 blocks, only 504 hours total), you can do:
	•	At the beginning of each block, fix the SoC to your desired “starting state” (50%, 25%, etc.).
	•	At the end of each block fix it to the same starting state. (symmetrical boundary conditions so the block doesn’t “cheat” by draining to 0% at the last hour.)

Since you’re ignoring multi-season transitions, you simply do something like:

# For block k in {Winter, Summer, Spring/Autumn}:
    E[block_start_hour] = E_initial  # e.g. half of emax
    E[block_end_hour] = E_initial    # or free, if you prefer

This ensures each block is standalone.

⸻

5. Extending the Existing Code

Below is a conceptual pipeline for your code. You can integrate it into your Python scripts (like planning.py, investment.py, etc.):
	1.	Read and Slice (data in data/processed)

full_load_df = pd.read_csv('load-2023.csv', parse_dates=['time'], index_col='time')
full_wind_df = pd.read_csv('wind-2023.csv', parse_dates=['time'], index_col='time')
full_solar_df = pd.read_csv('solar-2023.csv', parse_dates=['time'], index_col='time')

# Choose appropriate slices
winter_load = full_load_df.loc["2023-01-08":"2023-01-14"]  # 168h
summer_load = full_load_df.loc["2023-07-30":"2023-08-05"]
spring_autumn_load = full_load_df.loc["2023-10-22":"2023-10-28"]

# same for wind, solar
...


	2.	Build Single DataFrame
	•	If using Option B (only 3 blocks), you append them with a time index that has no overlap. Something like:

# Create a new "mega" index for 504 hours total
# (0..167 -> winter, 168..335 -> summer, 336..503 -> spring/autumn)

def reindex_to_block(original_df, block_start, block_name):
    # original_df has 168 hours
    # block_start is int, e.g. 0 for winter, 168 for summer, 336 for spring/autumn
    # return a new DataFrame with an integer range index from block_start..block_start+167
    new_index = range(block_start, block_start + 168)
    out = original_df.copy().reset_index(drop=True)
    out.index = new_index
    out['block'] = block_name
    return out

winter_block = reindex_to_block(winter_load, 0, "winter")
summer_block = reindex_to_block(summer_load, 168, "summer")
spring_autumn_block = reindex_to_block(spring_autumn_load, 336, "spring_autumn")

combined_load = pd.concat([winter_block, summer_block, spring_autumn_block], axis=0)
combined_load.sort_index(inplace=True)


	•	You’d do the same for wind and solar. Then you have combined_load, combined_wind, combined_solar each with 504 rows and a column “block” that indicates winter/summer/spring_autumn.

	3.	Define Weighting

def block_weight(block_name):
    if block_name == 'winter':
        return 13
    elif block_name == 'summer':
        return 13
    else: # spring_autumn
        return 26

combined_load['weight'] = combined_load['block'].apply(block_weight)
combined_wind['weight'] = combined_wind['block'].apply(block_weight)
combined_solar['weight'] = combined_solar['block'].apply(block_weight)

Or you can store the block weight in a separate structure.

	4.	Create “Generator” Data for DCOPF
	•	In your model, you probably have a DataFrame for each generator/time step. In this case, for each hour in [0..503], you have:
	•	pmin, pmax, gencost, etc.
	•	For wind or solar “generators,” you might treat them as:
	•	pmax[t] = wind_generation[t] (since it’s an upper bound that you cannot exceed)
	•	gencost = 0 if it’s a must-run or zero marginal cost resource
	•	For a conventional thermal generator, you have a standard cost. Then you do:

# Weighted cost
gencost[t] = original_cost * block_weight[t]  # incorporate weighting


	•	This can be done by simply multiplying the cost vector in your dcopf or investment_dcopf code by block_weight[t].

	5.	Storage Blocks
	•	For each block, add constraints:

E[s, block_start_hour] = some_initial_value
E[s, block_end_hour]   = the same or free


	•	If you want to fix to 50% SoC, you do so for hour 0 of each block.

	6.	Solve
	•	Now you pass the single 504-hour dataset to your existing solver.
	•	Internally, each hour’s cost is scaled by the block weight. The solver returns a single optimal solution that chooses how to dispatch each season.
	•	The “final cost” returned by the solver will approximate the full-year cost because the solver will see (Winter block cost × 13) + (Summer block cost × 13) + (Spring/Autumn block cost × 26).
	7.	Output & Reporting
	•	Because your timeseries is only 504 hours, you’ll have results for each hour.
	•	For a “whole year” summary, you can do:
	•	annual_cost = solution_cost (already includes weighting).
	•	For generator MWh: annual_MWh[g] = sum_over_block(generation[g, t]) * block_weight[t] if you want a final aggregator.
	•	If you want to plot a typical day or week, just remember the times are 0..503. You can re-map them back to “Day 1..7 of Winter,” “Day 1..7 of Summer,” “Day 1..7 of Spring/Autumn,” etc.

⸻

6. Implementation Outline

Below is a concise 8-step outline you can adapt into code:
	1.	Load & Merge Input Data
	•	Read year-long CSVs for load, wind, solar into DataFrames (with datetime index).
	2.	Extract Representative Weeks
	•	Slice the 3 distinct weekly intervals (168 hours each).
	3.	Build a Single 3-Block DataFrame
	•	Option B approach: Reindex each week to a disjoint integer time range, add a “block” column (winter/summer/spring_autumn).
	4.	Assign Weights
	•	For each block, define the factor (13, 13, 26).
	5.	Construct Model Inputs
	•	Bus data (unchanged)
	•	Branch data (unchanged)
	•	Generator data (some of which might be wind/solar with zero cost, but with pmax[t] = wind/solar potential).
	•	Make sure to multiply each hour’s generation cost by the block weight. For wind/solar, you can multiply the cost by the block weight if they have a cost, or keep it zero.
	•	For load, you just store the demand[t]. The solver cost calculations come from generation. (But if you do a load-shedding approach, you’d multiply the penalty by the block weight as well.)
	6.	Storage SoC Constraints
	•	For each block’s first hour, set E[s, first_hour_block] = e_initial (like 50% capacity).
	•	Decide if you also fix the final SoC or let it float.
	•	If your code expects a continuous timeseries, treat each block as if it starts right after the previous ends—but keep in mind you have no cross-block SoC constraints.
	7.	Run the DCOPF / Investment DCOPF
	•	The solver sees 504 hours total.
	•	In the objective function, each hour’s generation is multiplied by the block weight.
	•	The solver results in one solution that covers all hours. The final cost is the approximate annual cost.
	8.	Post-process
	•	Extract the solution for generation, flows, SoC, etc.
	•	For annual sums, do sum( flow[t] * block_weight[t] ), etc.
	•	For plotting, you can separate out hours 0..167 as “winter,” etc.

⸻

you can integrate this logic by create a new scripts that does the data slicing, weighting, and calls dcopf(investment=True, ...).

⸻

Conclusion

Using three seasonal representative weeks (winter, summer, spring/autumn) with weighting factors in the objective function is a proven way to approximate the full-year cost. It drastically reduces the computational burden compared to modeling all 8760 hours. You just need to carefully handle:
	•	Data slicing & indexing
	•	Objective weighting
	•	Storage boundary conditions

The steps outlined above should give you a robust framework. Let me know if anything needs adjustment or if you’d like clarifications on the weighting or data integration! Use planning - optimization and investment .py scripts as base for the integration.

Implement it step by step. No need to do all at once.