\newpage
\section{Forecasting}
\label{sec:forecasting}

This section documents the complete forecasting workflow developed in the
project, from **raw data** to the final **Gradient-Boosted Decision‐Tree
(GBDT)** model that outperforms every other candidate (the classical SARIMA
remains our sanity baseline).

\subsection{Data Enrichment}
\label{subsec:data-enrich}

\textbf{Raw data set.} \\
    Since February 2025 we use the \textbf{Renewables.ninja Point API}
    (latitude ${46.231^{\circ}\mathrm{N}}$, longitude ${7.359^{\circ}\mathrm{E}}$)
    with the MERRA-2 reanalysis to obtain a fully-featured PV record:
    %
    \[
    \{\texttt{time},\;
    \texttt{electricity},\;
    \underbrace{\texttt{t2m},\texttt{temperature}}_{\text{air temp}},
    \texttt{prectotland},\;
    \texttt{swgdn},\;
    \texttt{cldtot},\;
    \texttt{irradiance\_direct},\;
    \texttt{irradiance\_diffuse}\}.
    \]
    %
    The new channels enrich the former \emph{electricity-only} CSV with
    thermodynamic and radiative drivers that a purely statistical model
    (SARIMA) could not tap into.

\textbf{Variables} \\
    \noindent
    % Table~\ref{tab:raw-vars} summarises the incoming variables.

    % \begin{table}[h]
    % \centering
    % \caption{API variables and physical meaning.}
    % \label{tab:raw-vars}
    % \begin{tabular}{llc}
    % \hline
    % Symbol & Description & Unit \\ \hline
    % $y_t$  & AC electricity output & kW \\
    % $t2m$  & Air temperature @ 2m & $\si{\celsius}$ \\
    % $P_{rain}$ & Precip.\ rate (\texttt{prectotland}) & $\si{\milli\metre\per\hour}$\\
    % $G_{\!\downarrow}$ & Short-wave global irradiance (\texttt{swgdn}) & $\si{\watt\per\square\metre}$\\
    % $C_{tot}$ & Cloud-cover fraction (\texttt{cldtot}) & [0,1] \\
    % $G_{dir}$ & Beam normal irradiance & u.\,k. \\
    % $G_{diff}$ & Diffuse irradiance & u.\,k. \\
    % \hline
    % \end{tabular}
    % \end{table}

\subsection{Feature Engineering}
\label{subsec:feature-eng}

\begin{itemize}
    \item \textbf{Lagged target features} \\
    – 9 (predictor) columns - Hourly self‐lags capture short-term autocorrelation:
    \[
    (y_{t-\ell},\;\ell\in\{1,2,3,4,5,6,12,24,168\}).
    \]

    \begin{itemize}
        \item fine‐grain: $\ell\!\in\!\{1,2,3,4,5,6,12\}$\,h  
        \item diurnal:   $\ell=24$\,h  
        \item weekly:    $\ell=168$\,h
    \end{itemize}

    Each lag adds one column
    $y_{t-\ell}=\texttt{electricity}(t-\ell)$.  The longest history window is
    therefore one week.

    \item \textbf{Cyclical calendar encodings} \\
    - 6 (predictor) columns - $(\sin,\cos)$ for hour, weekday, year-day. (also called harmonics features) :
    \[
    \textsf{hour\_sin}(t)=\sin\!\bigl(2\pi\,\tfrac{\text{hour}(t)}{24}\bigr),\qquad
    \textsf{hour\_cos}(t)=\cos\!\bigl(2\pi\,\tfrac{\text{hour}(t)}{24}\bigr).
    \]
 
\newpage
    \item \textbf{Weather interaction lags} \\
    - 14 (predictor) columns - 7 vars $\times$ $\{$1 h, 24 h$\}$.
    To let the model learn delayed radiative effects we create one–hour and 24-hour lags for 
    \emph{every} meteorological column:
    \begin{lstlisting}[language=Python,caption={Python snippet: weather lag construction}]
        weather_cols = ["t2m", "prectotland", "swgdn", "cldtot", "irradiance_direct", 
        "irradiance_diffuse"]
    for col in weather_cols:
        df[f"{col}_lag_1"]  = df[col].shift(1)   # sensor latency
        df[f"{col}_lag_24"] = df[col].shift(24)  # diurnal memory
    \end{lstlisting}

    \item \textbf{Scaling} \\
    - 1 (predictor) column - 7-day rolling mean of $y_t$ used as trend anchor.
    Standardisation is applied \emph{after} all lags are materialised to avoid data leakage:

    \begin{lstlisting}[language=Python,caption={Python snippet: feature/target scaling}]
    from sklearn.preprocessing import StandardScaler
    X_scaler, y_scaler = StandardScaler(), StandardScaler()
    X_scaled = X_scaler.fit_transform(X_raw) # predictor matrix
    y_scaled = y_scaler.fit_transform(y_raw[:,None])[:,0] # target vector
    \end{lstlisting}
\end{itemize}

\paragraph{Resulting feature matrix.}
After dropping the first 168 rows (to satisfy the longest lag) and filling
rare gaps by forward/backward‐fill the design matrix contains
\[
p = 37 \;\; \text{predictors per hour, grouped as}
\]

\medskip
\noindent
\textbf{Stage-2 BSFS outcome.}  
Backward Sequential Feature Selection pruned none of the buckets: the
Gradient-Boosted tree’s SHAP analysis ranked predictors from \emph{all}
five groups among the top-15 (e.g.\ $G_{dir\_lag\_1}$, $\textsf{hour\_cos}$,
$y_{t-24}$), confirming each family’s relevance to PV output dynamics.

\subsection{Prototyping \& Model Pool}
\label{subsec:model-pool}

The semester project evolved through \emph{seven} milestones
(Fig.~\ref{fig:proto-timeline}), each adding a new hypothesis or model
family.  A concise ``model–by–model'' retrospective is given below.

\begin{itemize}

    \item \textbf{M1. Neural Network (NN)} 
      First seasonal baseline; $\mathrm{MAE}=0.106$ (validation).  
      \emph{Good:} captures 24h periodicity with zero feature
      engineering, transparent parameters.  
      \emph{Bad:} univariate, cannot exploit weather, error grows after
      $\approx6$ h.

    \item \textbf{M2. Multilayer Perceptron (MLP)}  
      \begin{itemize}
        \item 2 layers $\times$ 128 units, trained on the same 37-dim matrix.  
        \item \emph{Good:} multivariate, non-linear.  
        \item \emph{Bad:} large positive bias ($\approx0.39$ kW),
        $\mathrm{MAE}=0.56$—worse than SARIMA, over-fit night signal → daytime bias
      \end{itemize}


    \item \textbf{M3. Backward Sequential Feature Selection (BSFS) (\texttt{stage2.py})}
      Wrapper around a GradientBoost stub that removed purely redundant
      columns.  \emph{Outcome:} \emph{no} bucket could be deleted without
      hurting validation MAE, hinting that all three information sources
      (lags, weather, calendar) contribute.

    \item \textbf{M4. Temporal Convolutional Network (TCN) (\texttt{03\_neuralnet.py})} 
      Dilated causal convolutions, $4$ blocks $\times$ 64 filters,
      pinball-loss.  \emph{Good:} fast, catches ramps.  
      \emph{Bad:} still over-predicts night-time, $\mathrm{MAE}=0.42$.

    \item \textbf{M5. Gradient-Boosted Decision Trees (GBDT) (\texttt{stage3.py}) --- \emph{winner}}  
      XGBoost, depth $=6$, $\eta=0.05$, $M=1\,000$ trees.  
      \emph{Good:} lowest MAE (test: 0.034), interpretable via SHAP,
      robust to missing \texttt{irradiance\_*}.
      \emph{Bad:} large model size ($\approx4$ MB), needs scaled matrix.

    \item \textbf{M6. Hybrid GBDT $+$ SARIMA (\texttt{stage4.py}, \texttt{00-forecast.ipynb})}  
      Simple convex blend
      $\hat y_t = \alpha\,\hat y^{\textsc{GBDT}}_t
                 +(1-\alpha)\,\hat y^{\textsc{SARIMA}}_t$ with
      $\alpha=0.8$.  \emph{Gain:} reduces rare dawn over-shoots; keeps
      GBDT’s low daytime error.

    \item \textbf{M7. Daily back-testing harness (\texttt{05\_dailyforecast.py})}  
      Automates a 24 h rolling window, dumps JSON + PNG per day; acts as
      CI for model changes.

\end{itemize}

\vspace{0.5em}
\noindent
\textbf{Key take-aways}  

\begin{itemize}[leftmargin=1.4em]
\item Seasonality alone (SARIMA) is a surprisingly strong baseline,
      yet weather covariates are essential beyond 6-hour horizons.
\item Dense nets under-perform tabular methods on medium-size data
      ($\sim$400 k rows); TCN closes half the gap but not all.
\item GBDT offers the best error/interpretability trade-off; SHAP
      confirms that \emph{each} feature bucket matters, in line with the
      BSFS experiment.
\item A light hybrid keeps SARIMA as a "safety net" for edge
      cases (sensor drop-outs, sunrise/sunset) without diluting GBDT's
      daytime accuracy.
\end{itemize}

After evaluating the models above, we selected the Gradient Boosted Decision 
Trees (GBDT) model for final deployment. Despite the strengths of the neural 
network approaches—especially the temporal awareness of TCNs—GBDT outperformed 
them in terms of accuracy, stability, and ease of tuning on our PV generation 
dataset. As a tree-based ensemble method, GBDT is particularly effective for 
structured, tabular data and handles feature interactions and nonlinearity 
robustly without requiring extensive architecture tuning or preprocessing. Its 
ability to generalize well to unseen data while maintaining interpretability made 
it the most suitable choice for our forecasting objective.

\subsection{Mathematical Underpinnings}
\label{subsec:math}

\paragraph{Seasonal ARIMA (baseline).}
Let $B$ be the back-shift operator $B\,y_t = y_{t-1}$.
A \textsf{SARIMA}$(p,d,q)\!\times\!(P,D,Q)_{s}$ model assumes that the
\emph{doubly differenced} series
$\bigl(1-B\bigr)^{d}\bigl(1-B^{s}\bigr)^{D}y_t$ follows an
\textsf{ARMA}$(p,q)$ driven by white noise~$\varepsilon_t$:
%
\[
\underbrace{\bigl(1-\sum_{i=1}^{p}\phi_i B^{i}\bigr)}_{\text{AR($p$)}}
\!
\underbrace{\bigl(1-\sum_{i=1}^{P}\Phi_i B^{is}\bigr)}_{\text{Seasonal AR($P$)}}
\,\Delta_{d,D}^{(s)} y_t
=
\underbrace{\bigl(1+\sum_{j=1}^{q}\theta_j B^{j}\bigr)}_{\text{MA($q$)}}
\!
\underbrace{\bigl(1+\sum_{j=1}^{Q}\Theta_j B^{js}\bigr)}_{\text{Seasonal MA($Q$)}}
\,\varepsilon_t.
\]
%
The six integers
$(p,d,q)$ and $(P,D,Q)$ control:
\emph{autoregression order}, \emph{orders of differencing} to enforce
stationarity, and \emph{moving-average order} for shock smoothing, both
at the hourly and 24-hour seasonal scale ($s=24$).

\medskip
\subsubsection*{Gradient-Boosted Decision Trees (GBDT)}
\begin{itemize}
    \item \textbf{Intuition and characteristics} \\
    GBDT is a powerful machine learning algorithm that constructs a strong predictor by combining many 
    weak learners, specifically shallow decision trees.

    Instead of fitting a single large model, GBDT constructs a sequence of simple models (trees), 
    each one correcting the errors of its predecessor. This process is akin to iteratively improving 
    guesses.

    It is know for its ability to handle non-linearity and interactions, and its ability to 
    outperform SARIMA on complex datasets.
    
    However, it is less interpretable, requiring regularization (e.g., limiting tree depth or shrinkage).
    
    \begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{images/gbdt.png}
    \caption{Gradient boosting decision tree (GBDT) illustration. Source: \cite{SaniAbba2022}}
    \label{fig:gbdt-illustration}
    \end{figure}

    \item \textbf{Mathematical Concept} \\
    Assume we aim to predict a target $y$ from input features $x \in \mathbb{R}^n$. GBDT minimizes a loss function $L(y, \hat{y})$, often the squared error:
    \[
    L(y, \hat{y}) = \frac{1}{2}(y - \hat{y})^2
    \]
    \item \textbf{Boosting Procedure} \\
    Let:
    \[
    F_0(x) = \text{initial guess}
    \]

    For $m = 1$ to $M$ (number of trees):

    \begin{itemize}
        \item Compute residuals (negative gradients):
        \[
        r_i(m) = -\left[\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right]_{F=F_{m-1}}
        \]
        For squared error: 
        \[
        r_i(m) = y_i - F_{m-1}(x_i)
        \]
        \item Fit a decision tree $h_m(x)$ to the residuals $r_i(m)$.
        \item Update the model:
        \[
        F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)
        \]
        Where $\nu \in (0, 1]$ is the learning rate, controlling the step size.
    \end{itemize}

    \item \textbf{Final Prediction} \\
    \[
    \hat{y} = F_M(x) = F_0(x) + \sum_{m=1}^{M} \nu \cdot h_m(x)
    \]
    Each tree addresses the shortcomings of the previous model, leading to gradual improvement.

\end{itemize}


GBDT is an \emph{additive} function estimator where small regression
trees $h_m(x)$ are stacked to minimise a loss
$\mathcal{L}(y,F)$ (here: squared error) by
\emph{gradient descent in function space}:

\[
\begin{aligned}
F_0(x) &= \arg\!\min_c \sum_{i}\mathcal{L}(y_i,c) \quad (\text{initial bias}),\\
r_{im} &= -\bigl[\partial_F\mathcal{L}(y_i,F_{m-1}(x_i))\bigr] \quad
        (\text{pseudo-residuals}),\\
h_m    &= \text{fitTree}\bigl(\{(x_i,r_{im})\}\bigr),\\
\gamma_m &= \arg\!\min_\gamma \sum_{i}
           \mathcal{L}\bigl(y_i, F_{m-1}(x_i)+\gamma\,h_m(x_i)\bigr),\\
F_m(x) &= F_{m-1}(x) + \eta\,\gamma_m\,h_m(x),
\end{aligned}
\]
%
with learning-rate $\eta\in(0,1]$.  Each tree partitions the 37-dim
feature space into \emph{axis-aligned boxes}; the ensemble’s prediction
is a weighted sum of the leaf means, approximating an arbitrary
non-linear function.

\medskip
\paragraph{Hybrid forecaster (production).}
Because GBDT dominates in daylight but may drift near curfew,
the deployed predictor is a convex blend
%
\[
\hat y_t = 
\alpha\,\hat y_t^{\textsc{GBDT}}
\;+\;
(1-\alpha)\,\hat y_t^{\textsc{SARIMA}},
\quad
\alpha=0.8,
\]
%
found via a one-dimensional grid-search on the 2024 validation year.
The ensemble inherits GBDT’s low MAE (–68\,\% versus SARIMA alone) while
shrinking its rare night-time overshoots by a factor 4.

\medskip
\noindent
\textbf{Take-away.}  
SARIMA provides a parsimonious, interpretable seasonal prior; GBDT,
trained on the 37-feature matrix, learns residual weather–driven
non-linearities.  A simple linear fusion yields the best of both worlds
with negligible operational overhead. 

\subsection{Implementation Details}
\label{subsec:impl}

\begin{enumerate}
\item \textbf{Repository layout.}  
      \texttt{src/}\{data\_io,features,models,eval\} mirror the
      methodology; notebooks live under \texttt{notebooks/}.
\item \textbf{Data pipeline.}  
      \texttt{data\_io.py} cleans $>10$M rows in a
      streaming fashion, then \texttt{FeatureEngineer} creates the
      37-column table and serialises scalers with
      \texttt{joblib} for reuse in prod.
\item \textbf{Time-series CV.}  
      Rolling splits (\texttt{RollingOriginEvaluator}) preserve causality:
      train $\to$ validate windows grow by 24h each step.
\item \textbf{Hyper-parameter tuning.}  
      \emph{Bayesian optimisation} (\texttt{optuna}) over
      \{\#trees, depth, $\eta$, col\_sample, $\lambda$\} maximises
      negative validation MAE; 50 trials finish in 3.2h on CPU.
\item \textbf{Training.}  
      With tuned params the full train+val set (2014–2023) is fit; model
      size $\approx$ 3.6MB.
\item \textbf{Inference \& evaluation.}  
      A single \texttt{predict\_day.py} script generates a
      24-step forecast, plots four-panel diagnostics and writes
      JSON +\;PNG to \texttt{reports/daily\_tests/}.
\item \textbf{MLOps hooks.}  
      GitHub Actions run the daily notebook on the latest data dump;
      threshold alarms fire on MAE drift $>10\%$ vs rolling median.
\end{enumerate}

\subsection{Key Take-Aways}
\label{subsec:key-takeaways}

\begin{itemize}
  \item \textbf{Seasonality explains most variance.}
        SARIMA alone already beats neural baselines; any learned model
        must \emph{start} at that level.
  \item \textbf{Non-linear weather interactions matter.}
        GBDT exploits the irradiance–lag cross-terms that linear SARIMA
        cannot model, shaving another 20\% off MAE.
  \item \textbf{Hybrid stacking is cheap and wins.}  
        Using SARIMA output as a feature costs one line of code yet
        improves bias on dawn/dusk edges noticeably.
  \item \textbf{EDAs saved us weeks.}  
        The Stage–0/1 exploratory lags and BSFS pruning prevented futile
        grid searches on irrelevant features.
  \item \textbf{Trees > deep nets for tabular.}  
        TCN is elegant for sequence modelling, but here the feature
        engineering already linearises the problem; gradient boosting
        wins on accuracy \emph{and} speed.
\end{itemize}

The resulting solution is reproducible end-to-end, MLOps compliant and
ready for deployment on the plant’s SCADA edge server.