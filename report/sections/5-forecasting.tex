\newpage
\section{Forecasting}
\label{sec:forecasting}

In this project, the forecasting pipeline is structured not as a monolithic Python 
package, but as a set of iterative improvements and notebooks organized in a 
\texttt{notebooks/} directory:
\begin{lstlisting}[language=bash, numbers=none, backgroundcolor=\color{white}]
    notebooks/
      |-- setA.ipynb    # Time features only
      |-- setB.ipynb    # Lag + cyclic features
      |-- setC.ipynb    # Bayesian-optimized feature set
      |-- setD.ipynb    # Recursive prediction (failed)
      |-- setE.ipynb    # POA clear-sky and weather features
      `-- setF.ipynb    # XGBoost library
\end{lstlisting}
    
Each notebook implements and tests its own set of features or single 
“forecasting strategy,” incrementally building up from trivial baselines (like 
time-only) to advanced feature sets including weather derived ones. 

We want to forecast the electricity generation for the day ahead but will forecast on 7 days.
We can already see the cyclic behavior and the ramping behavior of the generation.
\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{images/week.png}
\caption{PV Generation normalized data (1 week)}
\label{fig:week}
\end{figure}

\subsection{Operational Forecasting Principle}
Forecasting in the operational sense means predicting the next $h$ hours 
(day-ahead or week-ahead) using only historical and current data available up to time 
$t$. Unlike “offline” statistical analyses, operational forecasts must avoid all data 
leakage: only lagged, cyclical, and weather-driven features that would be available 
before the forecast time can be used. 

The predictive model is retrained on the historical period. Then, it is rolled forward 
to generate and evaluate true out-of-sample predictions, mimicking a real operational 
workflow.

\subsection{Prototyping \& Machine Learning Models}
\label{subsec:model-pool}

To benchmark our operational forecasting framework, we used MAE as main metric but also 
RMSE and R2. They can be interpreted as follow :
\vspace{-0.4cm}
\begin{itemize}
    \item MAE: Mean Absolute Error, interpreted in the same units as the target variable. 
    It is the average absolute difference between the predicted and actual values.
    \item RMSE: Root Mean Square Error, penalizes big mistakes. It is the square root of 
    the average of the squares of the errors.
    \item R2: R-squared, measures how well the model explains the variance in the target 
    variable. It assesses the overall fit.
\end{itemize}

We required all machine learning models to outperform a SARIMA baseline. 
Although we began with neural networks, we ultimately switched to gradient boosted trees 
for their interpretability and practical strengths with engineered time features. Detailed below,
both modeling approaches and the feature design supporting them.

\subsubsection*{Neural Network Models}
We iterated on the following models:
\begin{enumerate}
  \item \textbf{Simple Neural Network (NN)}\\
        A neural network is a computational model inspired by the human brain. 
        Our simple model consisted of a single layer of interconnected nodes (neurons) 
        that process data by applying weights, biases, and activation functions.
        Predictions are made from "patterns" learned during training.
        \vspace{-0.4cm}

        \emph{Bad:} fails to capture strong seasonality $\rightarrow$ high bias.
  \item \textbf{Multilayer Perceptron (MLP)}\\
        MLP is a specific type of neural network: a fully connected feedforward neural 
        network with one or more hidden layers and nonlinear activation functions.
        We searched (tuned) for the optimal number of layers and units within an array of 1 
        to 4 layers and 1 to 128 units.

        \emph{Good:} smooth forecasts; handles non-linearities.\\
        \emph{Bad:} night-time over-fit (not 0) despite target scaling tricks.
  \item \textbf{Temporal Convolutional Network (TCN)}\\
        TCN is a type of neural network designed for sequential data. It uses 1D causal 
        convolutions to capture temporal dependencies and are particularly effective for 
        handling long-term dependencies.

        \emph{Good:} tracks ramps well; long memory.\\
        \emph{Bad:} 3--5× GPU time, sensitive to feature scaling, and still
        overshoots zero generation at night.
\end{enumerate}

We explicitly set $y_t=0$ between dusk and dawn (computed from the
solar-zenith angle in the weather feed) to remove the night signal forecasting errors.
Although this removed the worst negative bias, the best TCN still landed at \textrm{MAE}$=4.2\%$ and required
$>\!90$ min of hyper-parameter tuning. This was not a good trade off.

\subsubsection*{Pivot to Ensemble Trees}  
Gradient Boosted Decision Trees (GBDT) is an ensemble machine learning method. Meaning that in the 
end, all the trees are combined to make one powerful model where each new tree tries to reduce 
the errors (called Loss) made by the previous ones. This error is minimized via a technique called
gradient descent who looks at how the error changes and then adjsuts the next tree based on it.

\begin{figure}[h!]
\centering
\includegraphics[width=0.5\textwidth]{images/gbdt.png}
\caption{Gradient boosting decision tree (GBDT) illustration. Source: \cite{SaniAbba2022}}
\label{fig:gbdt-illustration}
\end{figure}

GBDTs differ fundamentally from neural networks. They are non-parametric and rule-based, 
meaning they do not require extensive scaling or normalization of inputs and are typically 
much faster and lighter to train. Their main tunable hyperparameters include the number of 
trees, the maximum depth per tree, the learning rate, and regularization settings.

For model development, we split our data into three sets: training, validation, and test. 
The training set is used to fit the model, the validation set guides hyperparameter tuning 
and prevents overfitting, and the final evaluation is performed on the test set (our 7days 
of forecast -- 1st day being the main target).

Tree ensembles have distinct strengths: they tend to be more interpretable, naturally 
handle tabular (structured) data, and are robust to irrelevant features or collinearity. 
However, unlike neural networks that can automatically model sequential dependencies, 
GBDTs require us to explicitly engineer time-based features (such as lags and calendar 
variables) to capture temporal effects.

In summary, the switch to GBDT was motivated by both practical considerations and 
empirical results. Tree ensembles offered consistently higher accuracy, much faster 
training, and easier interpretability than neural networks in our forecasting task. 
This made GBDT the clear choice for our prototyping and final model selection.

\subsection{Feature Engineering}

Feature engineering is a critical step in time-series forecasting models and gradient 
boosted trees in particular, as it enables the extraction of temporal patterns, cyclic 
behavior, and relevant dependencies from the raw data. In this project, feature 
engineering focused on three main strategies: time features (cyclical encoding), 
lagged features, and "engineered" weather/irradiance features.

\subsubsection*{A. Cyclical Time Features}
Hourly, daily, and seasonal periodicity in electricity generation and demand is well-known. 
To encode these periodic effects, we transformed the raw time variables into their 
cyclical (sin/cosine) representations:
\[
\begin{aligned}
\text{hour\_sin}  &= \sin\left(2\pi \cdot \frac{\text{hour}}{24}\right) 
\quad \text{,} \quad 
\text{hour\_cos}  = \cos\left(2\pi \cdot \frac{\text{hour}}{24}\right) \\
\end{aligned}
\]
This encoding allows the model to learn periodicity without artificial jumps between 
boundary values (e.g., hour 23 to hour 0). The 'hour' feature can be replaced by the 
month, day of week and day of year in our equation.

\subsubsection*{B. Lag Features}
To capture autocorrelation and persistence effects in the target time series, we 
included lagged values of the target variable (electricity generation) as additional 
features. For each time $t$, we added the previous values at selected time intervals:
\[
\text{electricity\_lag}_\ell[t] = y_{t-\ell}
\]
where $y_t$ denotes the target value at time $t$, and $\ell$ is the lag in hours.

In the final feature set, we included lags of $\ell \in \{1,\, 2,\, 3,\, 6,\, 
12,\, 24,\, 48,\, 168\}$, corresponding to 1 hour up to 1 week (168 hours), i.e.:
\[
\mathcal{L} = \{1, 2, 3, 6, 12, 24, 48, 168\}
\]
This means the model, at each prediction time, has access to the target value for up 
to the past 7 days.

\subsubsection*{C. Weather and Irradiance Features}
For the advanced models (e.g., Set E and Set F in our code), we engineered domain-specific 
features using both direct measurements and physical models:
\begin{itemize}
    \item Plane of Array (POA) Irradiance: calculated with the \texttt{pvlib} library using 
    local site, solar position parameters and the weather data from the 
    \texttt{Renewables.ninja} API. The physical formula is :
    \begin{equation}
    G_{\text{POA}} = G_b \cdot R_b + G_d \cdot F_d + G_h \cdot \rho_g \cdot \frac{1 - \cos(\beta)}{2}  
    \end{equation}
    % G_{\text{beam}} + G_{\text{diffuse}} + G_{\text{reflected}  
    where:
    \begin{itemize}
        \item $G_b$ = direct normal irradiance (DNI)
        \item $R_b$ = geometric factor for beam component (depends on solar and panel angles)
        \item $G_d$ = diffuse horizontal irradiance (DHI)
        \item $F_d$ = view factor for diffuse component (depends on tilt)
        \item $G_h$ = global horizontal irradiance (GHI)
        \item $\rho_g$ = ground reflectance (albedo)
        \item $\beta$ = tilt angle of the panel
    \end{itemize}
 \item Clear-sky Index: computed as the ratio between measured POA and modeled 
    clear-sky POA :
    \begin{equation}
    \text{POA clear-sky index}[t] = \frac{\text{poa\_total}[t]}{\text{poa\_clearsky}[t]}
    \end{equation}
\end{itemize}  
For both the target and key weather features (especially POA clear-sky index), we also 
included recent lags (e.g., 1h, 2h, 3h, 6h, 12h, 24h) and 24h rolling means.
\[
\text{poa\_clearsky\_index\_lag}_k[t] = \text{poa\_clearsky\_index}[t-k]
\]

\subsubsection*{D. Maximum Lag and Buffering}
A critical implementation detail is the handling of missing data at the start of each 
split due to lagging. For any feature with maximum lag $\ell_{max}$, at least 
$\ell_{max}$ time steps are dropped at the beginning of each set (e.g. :
\vspace{-0.4cm}
\begin{itemize}
    \item $\ell_{max}=168$ for target lags in most models : "7 days of data are needed before the first prediction"
\end{itemize}
\vspace{-0.4cm}
During forecasting, a lag buffer window is reserved so that the first prediction is always 
made with valid historical context.

\subsubsection*{E. Feature Set Size and Selection}
Adding engineered features significantly increases the dimensionality of the input 
data matrix. While richer features can boost model accuracy, including too many can 
introduce noise or lead to overfitting. To ensure our models focused on the most 
relevant information, we performed feature selection prior to training.

Several approaches exist for feature selection, such as forward selection (adding 
features one at a time) or backward elimination (removing the less important features step
by step). In our workflow, we used a more efficient method: Bayesian optimization.
\cite{sklearn_feature_selection}. It searches for the subset of features (and 
hyperparameters) that minimize validation error, reducing computational burden and 
maximizing forecast skill.


\subsection{Mathematical Background}
\label{subsec:math}

\subsubsection*{A. Statistical Model - Baseline}

We used SARIMA a statistical model as a benchmark. It applies ARMA modeling on a 
transformed (differenced) version of the time series to capture both short-term 
dynamics and repeating seasonal patterns. Its power lies in modeling both the temporal 
structure and seasonal cycles within a single, compact framework.

\textbf{ARMA} $(p, q)$ is a linear combination of two things and works only on 
stationary data. It means the time series must have constant mean and variance over 
time. It is written as:
\begin{itemize}
    \item an \emph{autoregressive (AR)} part of order $p$ $\rightarrow$ how past values influence the present
    \item a \emph{moving average (MA)} part of order $q$ $\rightarrow$ how past errors influence the present
\end{itemize}
The ARMA model is written as:
$$
y_t = c + \sum_{i=1}^{p} \phi_i y_{t-i} + \varepsilon_t + \sum_{j=1}^{q} \theta_j \varepsilon_{t-j}
$$
\begin{itemize}
    \item $c$: constant term (mean of the process, if not differenced)
    \item $\phi_i$: AR coefficients
    \item $\varepsilon_t$: white noise error term at time $t$
    \item $\theta_j$: MA coefficients
\end{itemize}

\textbf{SARIMA} $(p,d,q)\times(P,D,Q)_s$ extends ARMA to handle trends and 
seasonality, in two ways: 
\begin{enumerate}
    \item \textbf{Differencing} for trend removal \\
        To remove non-stationary trends, SARIMA applies ordinary differencing $d$ times:
        \[
        y_t' = (1 - L)^d y_t = y_t - y_{t-1} \quad \text{(if } d = 1\text{)}
        \]
    \item \textbf{Seasonal differencing} for repeating patterns \\
        To remove seasonal effects (e.g., daily or yearly patterns), it applies 
        seasonal differencing $D$ times with period $s$:
        \[
        y_t'' = (1 - L^s)^D y_t' = y_t' - y_{t-s}' \quad \text{(if } D = 1\text{)}
        \]
\end{enumerate}

The six hyperparameters $(p,d,q)$ and $(P,D,Q)$ define the orders of autoregression, 
differencing, and moving-average smoothing at both the regular (hourly) and seasonal 
(daily) levels, with $s = 24$ reflecting the daily cycle. If the model residuals 
resemble white noise—i.e., they are uncorrelated and pattern-free—the model is 
considered to have successfully extracted all systematic, predictable structure.

\subsubsection*{B. Machine Learning Model - GBDT}
Assume we aim to predict a target $y$ from input features $x \in \mathbb{R}^n$. 
GBDT minimizes a loss function $L(y, \hat{y})$. (Minimizing mean absolute error via 
gradient descent between a predicted $\hat{y}$ and true $y$ value).

Let:
\[
F_0(x) = \text{initial guess} \quad\quad \text{for } m = 1 \text{ to } M
\]

Boosting builds many shallow trees \emph{sequentially}.  
Each tree tries to predict the \emph{errors} the previous trees made:
    \[
    r_i^{(m)} = y_i - F_{m-1}(x_i)
    \]
At each boosting step $m$, we compute the residual $r_i^{(m)}$, which is the difference 
between the true value $y_i$ and the current model’s prediction $F_{m-1}(x_i)$.
This residual guides the new tree $h_m$ to correct errors made so far.

The new model $F_m$ is updated by adding a fraction $\nu$ $\in (0, 1]$ (learning rate, 
controlling the step size) of the new tree $h_m(x)$ to the previous model’s output.
This sequential additive approach allows gradual improvement while avoiding overfitting.
    \[
    F_m(x) = F_{m-1}(x) + \nu\,h_m(x)
    \]
After $M$ trees, the final prediction is:
    \[
    \hat{y} = F_M(x) = F_0(x) + \sum_{m=1}^{M} \nu \cdot h_m(x)
    \] 
The final prediction $\hat{y}$ is the initial model $F_0(x)$ (often a constant 
like the mean of $y$) plus the sum of all $M$ trees' predictions scaled by $\nu$.
This ensemble aggregates weak learners into a strong one. 

Because every tree focuses on what is still unexplained, the ensemble
gradually improves until additional trees no longer cut the validation loss.

\subsection{Implementation Workflow}
\label{subsec:implementation}

The full forecasting workflow was implemented in Python using open-source libraries 
(\texttt{pandas}, \texttt{scikit-learn}, \texttt{xgboost}, etc.), following a 
reproducible, modular structure. Each notebook (\texttt{setA}–\texttt{setF}) 
developed and tested a distinct feature set or model. The core implementation steps 
are as follows:

\begin{enumerate}
    \item \textbf{Data Loading:} Import and preprocess the raw data, including handling timestamps and missing values.
    \item \textbf{Feature Engineering:} Generate cyclical time features, target and weather lags, 
    rolling means, and any additional domain-specific variables.
    \item \textbf{Data Splitting:} Partition the dataset into training, validation, and test sets, 
    reserving lag buffers to avoid information leakage.
    \item \textbf{Feature Selection:} Use methods such as SelectKBest or Bayesian optimization 
    to identify the most informative subset of features by minimizing the validation error (MAE).
    \item \textbf{Hyperparameter Tuning:} Optimize model hyperparameters (e.g., number of trees, 
    learning rate, max depth) using cross-validation.
    \item \textbf{Validation and Evaluation:} Evaluate the model on the validation and test sets using error metrics.
    \item \textbf{Analysis:} Analyze feature importance, daily error metrics, and compare model performance to baseline.
\end{enumerate}

This modular workflow enables systematic experimentation, rigorous benchmarking, and easy 
extension with new features or algorithms.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[node distance=1.2cm, every node/.style={font=\footnotesize}]
    % Nodes
    \node (data) [draw, rounded corners, fill=blue!10] {Raw Data};
    \node (prep) [draw, rounded corners, below=of data, fill=gray!10] {Preprocessing};
    \node (features) [draw, rounded corners, below=of prep, fill=yellow!10] {Feature Engineering};
    \node (split) [draw, rounded corners, below=of features, fill=orange!10] {Train / Val / Test Split (with Lag Buffer)};
    \node (fselect) [draw, rounded corners, right=3cm of features, fill=yellow!20] {Feature Selection};
    \node (htune) [draw, rounded corners, below=of fselect, fill=red!10] {Hyperparameter Tuning (CV)};
    \node (model) [draw, rounded corners, below=of split, fill=green!10] {Model Training};
    \node (val) [draw, rounded corners, below=of model, fill=purple!10] {Validation / Evaluation};
%    \node (recursive) [draw, rounded corners, below=of val, fill=cyan!10] {Recursive Forecasting};
    \node (output) [draw, rounded corners, below=of val, fill=cyan!10] {Results \& Analysis};

    % Arrows
    \draw[->] (data) -- (prep);
    \draw[->] (prep) -- (features);
    \draw[->] (features) -- (split);
    \draw[->] (split) -- (model);
    \draw[->] (model) -- (val);
    \draw[->] (val) -- (output);

    % Feature Selection and Hyperparameter Tuning Branch
    \draw[->] (features) -- (fselect);
    \draw[->] (fselect) -- (htune);
    \draw[->] (htune) |- (model);

    Legend/Side notes (optional)
    \node[align=left, right=4.3cm of output, font=\scriptsize] (legend) {%
        \textbf{Notes} \\
        - All steps modular \\
        - Recursive forecasting not implemented \\
        - Easy to swap models / features
    };

\end{tikzpicture}
\caption{End-to-end forecasting pipeline: from raw data and feature engineering, through 
feature selection and model training, to evaluation.}
\label{fig:forecasting-workflow}
\end{figure}