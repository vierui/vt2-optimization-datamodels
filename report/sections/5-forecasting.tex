\newpage
\section{Forecasting}
\label{sec:forecasting}

This section walks through the full forecasting pipeline, starting with the
\emph{raw} data retrieved and ending with the \textbf{Gradient-Boosted Decision-Tree 
(GBDT)} model that ultimately ships to production.  Each stage—data enrichment, 
feature engineering, model prototyping, and mathematical grounding—is documented 
so that a future engineer can reproduce (or challenge) every decision.  The 
classical seasonal ARIMA (SARIMA) remains our statistical “baseline” and provides 
the first sanity check for all machine-learning attempts.

\subsection{Data Enrichment}
\label{subsec:data-enrich}

The investment model framework we previously developed offers \textbf{one year of 
hourly energy demand} that we opt to use as a static baseline and scale per asset.  
For the \emph{forecasting} task, however, a richer data-context is
essential.

\begin{enumerate}[leftmargin=1.2em]
  \item \textbf{Meteorological history}\\  
        We query the \emph{Renewables.ninja} Point API
        (lat.\,$46.231^{\circ}\mathrm{N}$, lon.\,$7.359^{\circ}\mathrm{E}$,
        Sion) with the MERRA-2 re-analysis and retrieve \textbf{11 complete
        years} of hourly weather and PV simulation, January 2014 – December 2024.
  \item \textbf{New data set} \\ 
        The original \{\texttt{time}, \texttt{electricity}\} table now includes
        temperature, rain rate, global and diffuse irradiance, cloud cover, and
        beam normal irradiance such as : 
    
        \begin{table}[h]
        \centering
        \caption{API variables and physical meaning.}
        \label{tab:raw-vars}
        \begin{tabular}{llc}
        \hline
        Symbol & Description & Unit \\ \hline
        $y_t$  & AC electricity output & kW \\
        $t2m$  & Air temperature @ 2m & \degree C \\
        $P_{rain}$ & Precip.\ rate (\texttt{prectotland}) & mm/h \\
        $G_{\!\downarrow}$ & Short-wave global irradiance (\texttt{swgdn}) & W/m\textsuperscript{2} \\
        $C_{tot}$ & Cloud-cover fraction (\texttt{cldtot}) & [0,1] \\
        $G_{dir}$ & Beam normal irradiance & \text{W/m\textsuperscript{2}} \\
        $G_{diff}$ & Diffuse irradiance & \text{W/m\textsuperscript{2}} \\
        \texttt{T} & Air temperature & \degree C \\
        \hline
        \end{tabular}
        \end{table}

        This means that we now have a data set with 9 features and 11 years of hourly data.
        The table now looks like this :
        \[
            \bigl\{
            time , y_t,\ t2m,\ P_{\text{rain}},\ G_{\!\downarrow},\ C_{tot},\
            G_{dir},\ G_{diff}, T, \bigr\}.
        \]
\end{enumerate}

Some variables feed the model directly; others only serve as building 
blocks during feature engineering. Data quality was also assessments were
also performed.


\subsection{Feature Engineering}
\label{subsec:feature-eng}
Feature engineering involves transforming raw time series data into informative 
inputs—such as lag values, rolling averages, or seasonal indicators—that help 
models capture patterns like trend and seasonality.

For SARIMA, features are built into the model via differencing and seasonal terms, 
while in machine learning, these engineered features explicitly guide the learning
process to improve prediction accuracy. Features can be grouped into different groups : 

\begin{itemize}
    \item \textbf{Lagged target features} \\
    9 additional columns - Hourly self‐lags capture short-term autocorrelation:
    \[
    (y_{t-\ell},\;\ell\in\{1,2,3,4,5,6,12,24,168\}).
    \]

    \begin{itemize}
        \item fine‐grain: $\ell\!\in\!\{1,2,3,4,5,6,12\}$\,h  
        \item diurnal:   $\ell=24$\,h  
        \item weekly:    $\ell=168$\,h
    \end{itemize}

    Each lag adds one column
    $y_{t-\ell}=\texttt{electricity}(t-\ell)$.  The longest history window is
    therefore one week.

    \item \textbf{Cyclical calendar encodings} \\
    6 additional columns - $(\sin,\cos)$ for hour, weekday, year-day. Commonly called harmonics features:
    \[
    \textsf{hour\_sin}(t)=\sin\!\bigl(2\pi\,\tfrac{\text{hour}(t)}{24}\bigr),\qquad
    \textsf{hour\_cos}(t)=\cos\!\bigl(2\pi\,\tfrac{\text{hour}(t)}{24}\bigr).
    \]
 
    \item \textbf{Weather interaction lags} \\
    14 additional columns - 7 vars $\times$ $\{$1 h, 24 h$\}$.
    To let the model learn delayed radiative effects we create one–hour and 24-hour lags for 
    \emph{every} meteorological column:
    \begin{lstlisting}[language=Python,caption={Python snippet -- weather lag construction}]
        weather_cols = ["t2m", "prectotland", "swgdn", "cldtot", "irradiance_direct", 
        "irradiance_diffuse"]
    for col in weather_cols:
        df[f"{col}_lag_1"]  = df[col].shift(1)   # sensor latency
        df[f"{col}_lag_24"] = df[col].shift(24)  # diurnal memory
    \end{lstlisting}

    \item \textbf{Scaling} \\
    1 additional column - 7-day rolling mean of $y_t$ used as trend anchor.
    Standardisation is applied \emph{after} all lags are materialised to avoid data leakage:

    \begin{lstlisting}[language=Python,caption={Python snippet -- feature/target scaling}]
    from sklearn.preprocessing import StandardScaler
    X_scaler, y_scaler = StandardScaler(), StandardScaler()
    X_scaled = X_scaler.fit_transform(X_raw) # predictor matrix
    y_scaled = y_scaler.fit_transform(y_raw[:,None])[:,0] # target vector
    \end{lstlisting}
\end{itemize}

\paragraph{Resulting feature matrix.}
After dropping the first 168 rows (to satisfy the longest lag) and filling
rare gaps by forward/backward‐fill the design matrix contains
\[
p = 37 \;\; \text{predictors per hour, grouped as}
\]

\paragraph{Backward Sequential Feature Selection (BSFS).}%
\label{par:bsfs}
Stage-2 BSFS removes entire \emph{groups}, not individual columns, until the
validation MAE stops decreasing.  In practice \emph{all five} buckets
(lags, calendar, weather lags, trend scaler, raw weather) survive—
confirmation that each family explains a distinct slice of variance.


\subsection{Prototyping \& Machine Learning Models}
\label{subsec:model-pool}

To evaluate the performance of the models we used the Mean Absolute Error (MAE) as a metric.
A statistical benchmark SARIMA sets the baseline and every ML model should first beat it.
Our original goal was a neural-network solution, so we iterated through increasingly
sophisticated architectures but ended up switching to tree ensembles because of their 
interpretability insensitivity to feature scaling and overfitting.

\subsubsection*{Neural Network Models}

\begin{enumerate}
  \item \textbf{Simple Neural Network (NN)}\\
        A neural network is a computational model inspired by the human brain. 
        In our model, it consisted of a single layer of interconnected nodes (neurons) 
        that process data by applying weights, biases, and activation functions.
        Predictions are made from patterns learned during training.

        \emph{Bad:} fails to capture strong seasonality $\rightarrow$ high bias.
  \item \textbf{Multilayer Perceptron (MLP)}\\
        MLP is a specific type of neural network: a fully connected feedforward neural 
        network with one or more hidden layers and nonlinear activation functions.
        We searched (tuned) for the optimal number of layers and units within an array of 1 
        to 4 layers $\times$ 1 to 128 units, trained on the same 37-dim matrix.

        \emph{Good:} smooth forecasts; handles non-linearities.\\
        \emph{Bad:} night-time over-fit despite target scaling tricks.
  \item \textbf{Temporal Convolutional Network (TCN)}\\
        TCN is a type of neural network designed for sequential data. It uses 1D causal 
        convolutions to capture temporal dependencies and are particularly effective for 
        handling long-term dependencies.

        \emph{Good:} tracks ramps well; long memory.\\
        \emph{Bad:} 3× GPU time, sensitive to feature scaling, and still
        overshoots zero generation at night.
\end{enumerate}


We explicitly set $y_t=0$ between civil dusk and dawn (computed from the
solar-zenith angle in the weather feed) to remove the night signal forecasting errors.
Although this removed the worst negative bias, the best TCN still landed at \textrm{MAE}$=4.2\%$ and required
$>\!90$ min of hyper-parameter tuning. This was not a good trade off.

\subsubsection*{Pivot to Ensemble Trees}  
Gradient Boosted Decision Trees (GBDT) is an ensemble machine learning method. meaning in the 
end, all the trees are combined to make one powerful model where each new tree tries to reduce 
the errors (called Loss) made by the previous ones. This error is minimized via a technique called
gradient descent who looks at how the error changes and then adjsuts the next tree based on it.

\begin{figure}[h!]
\centering
\includegraphics[width=0.5\textwidth]{images/gbdt.png}
\caption{Gradient boosting decision tree (GBDT) illustration. Source: \cite{SaniAbba2022}}
\label{fig:gbdt-illustration}
\end{figure}

Contrary to Neural Networks, GBDT are then non-parametric, rule-based. They are then lighter and 
faster to train. The tunnable values are the number of trees, the depth of the trees, the learning 
rate and the regularization parameter.

The data is split into three parts: the training set, the validation set and the test set. The training set is 
used to train the model and the validation set is used to evaluate the model. The model is then 
evaluated on the validation set and the process is repeated until the model is good enough. 
The model is then used to make predictions on the test set. The test set is used to evaluate the model.

GBDT are usually more interpretatabel and excel with tabular data (structured datasets). Howwever, 
GBDT does not inherently understand time or sequence, unless time features are manually engineered.

GBDTs use splits on feature values while TCNs use convolutions over time to model dependencies.


% \textbf{Key take-aways}  

% \begin{itemize}[leftmargin=1.4em]
% \item Seasonality alone (SARIMA) is a surprisingly strong baseline,
%       yet weather covariates are essential beyond 6-hour horizons.
% \item Dense nets under-perform tabular methods on medium-size data
%       ($\sim$400 k rows); TCN closes half the gap but not all.
% \item GBDT offers the best error/interpretability trade-off; SHAP
%       confirms that \emph{each} feature bucket matters, in line with the
%       BSFS experiment.
% \item A light hybrid keeps SARIMA as a "safety net" for edge
%       cases (sensor drop-outs, sunrise/sunset) without diluting GBDT's
%       daytime accuracy.
% \end{itemize}

Despite the initial appeal of using neural networks like TCNs for their temporal 
awareness, the lack of familiarity with advanced libraries such as TensorFlow led 
to unsatisfying results. This prompted a return to the basics using scikit-learn and 
an exploration of GBDT, supported by literature highlighting its strengths.

GBDT emerged as the most suitable model due to its strong performance on tabular data,
ease of capturing seasonality with calendar features, and superior accuracy and 
training efficiency compared to TCNs. Its minimal tuning requirements and further 
reinforced its suitability for the forecasting task.

\subsection{Mathematical Underpinnings}
\label{subsec:math}

\subsubsection*{Statistical Model - Baseline}

We used SARIMA a statistical model as a benchmark. It applies ARMA modeling on a 
transformed (differenced) version of the time series to capture both short-term 
dynamics and repeating seasonal patterns. Its power lies in modeling both the temporal 
structure and seasonal cycles within a single, compact framework.

\textbf{ARMA} $(p, q)$ is a linear combination of two things and works only on 
stationary data. It means the time series must have constant mean and variance over 
time. It is written as:
\begin{itemize}
    \item an \emph{autoregressive (AR)} part of order $p$ $\rightarrow$ how past values influence the present
    \item a \emph{moving average (MA)} part of order $q$ $\rightarrow$ how past errors influence the present
\end{itemize}
The ARMA model is written as:
$$
y_t = c + \sum_{i=1}^{p} \phi_i y_{t-i} + \varepsilon_t + \sum_{j=1}^{q} \theta_j \varepsilon_{t-j}
$$
\begin{itemize}
    \item $c$: constant term (mean of the process, if not differenced)
    \item $\phi_i$: AR coefficients
    \item $\varepsilon_t$: white noise error term at time $t$
    \item $\theta_j$: MA coefficients
\end{itemize}

\textbf{SARIMA} $(p,d,q)\times(P,D,Q)_s$ extends ARMA to handle trends and 
seasonality, in two ways: 
\begin{enumerate}
    \item \textbf{Differencing} for trend removal \\
        To remove non-stationary trends, SARIMA applies ordinary differencing $d$ times:
        \[
        y_t' = (1 - L)^d y_t = y_t - y_{t-1} \quad \text{(if } d = 1\text{)}
        \]
    \item \textbf{Seasonal differencing} for repeating patterns \\
        To remove seasonal effects (e.g., daily or yearly patterns), it applies 
        seasonal differencing $D$ times with period $s$:
        \[
        y_t'' = (1 - L^s)^D y_t' = y_t' - y_{t-s}' \quad \text{(if } D = 1\text{)}
        \]
\end{enumerate}

The six hyperparameters $(p,d,q)$ and $(P,D,Q)$ define the orders of autoregression, 
differencing, and moving-average smoothing at both the regular (hourly) and seasonal 
(daily) levels, with $s = 24$ reflecting the daily cycle. If the model residuals 
resemble white noise—i.e., they are uncorrelated and pattern-free—the model is 
considered to have successfully extracted all systematic, predictable structure.

\subsubsection*{Machine Learning Model - GBDT}
Assume we aim to predict a target $y$ from input features $x \in \mathbb{R}^n$. 
GBDT minimizes a loss function $L(y, \hat{y})$. (Minimizing mean absolute error via 
gradient descent between a predicted $\hat{y}$ and true $y$ value).

Let:
\[
F_0(x) = \text{initial guess} \quad\quad \text{for } m = 1 \text{ to } M
\]

Boosting builds many shallow trees \emph{sequentially}.  
Each tree tries to predict the \emph{errors} the previous trees made:
    \[
    r_i^{(m)} = y_i - F_{m-1}(x_i)
    \]
At each boosting step $m$, we compute the residual $r_i^{(m)}$, which is the difference 
between the true value $y_i$ and the current model’s prediction $F_{m-1}(x_i)$.
This residual guides the new tree $h_m$ to correct errors made so far.

The new model $F_m$ is updated by adding a fraction $\nu$ $\in (0, 1]$ (learning rate, 
controlling the step size) of the new tree $h_m(x)$ to the previous model’s output.
This sequential additive approach allows gradual improvement while avoiding overfitting.
    \[
    F_m(x) = F_{m-1}(x) + \nu\,h_m(x)
    \]
After $M$ trees, the final prediction is:
    \[
    \hat{y} = F_M(x) = F_0(x) + \sum_{m=1}^{M} \nu \cdot h_m(x)
    \] 
The final prediction $\hat{y}$ is the initial model $F_0(x)$ (often a constant 
like the mean of $y$) plus the sum of all $M$ trees' predictions scaled by $\nu$.
This ensemble aggregates weak learners into a strong one. 

Because every tree focuses on what is still unexplained, the ensemble
gradually improves until additional trees no longer cut the validation loss.


Why GBDT wins here? With 37 engineered predictors the relationship to $y_t$ is mostly
\textit{piece-wise} and \textit{non-linear}. Rule-based splits excel at that,
while requiring almost no architecture design—only \{\#trees, depth,~$\nu$\}
need tuning. The GBDT is then a good choice because it is non-parametric, rule-based, 
light and fast to train.

% with learning-rate $\eta\in(0,1]$.  Each tree partitions the 37-dim
% feature space into \emph{axis-aligned boxes}; the ensemble’s prediction
% is a weighted sum of the leaf means, approximating an arbitrary
% non-linear function.

% \medskip
% \paragraph{Hybrid forecaster (production).}
% Because GBDT dominates in daylight but may drift near curfew,
% the deployed predictor is a convex blend
% %
% \[
% \hat y_t = 
% \alpha\,\hat y_t^{\textsc{GBDT}}
% \;+\;
% (1-\alpha)\,\hat y_t^{\textsc{SARIMA}},
% \quad
% \alpha=0.8,
% \]
% %
% found via a one-dimensional grid-search on the 2024 validation year.
% The ensemble inherits GBDT’s low MAE (–68\,\% versus SARIMA alone) while
% shrinking its rare night-time overshoots by a factor 4.

% \medskip
% \noindent
% \textbf{Take-away.}  
% SARIMA provides a parsimonious, interpretable seasonal prior; GBDT,
% trained on the 37-feature matrix, learns residual weather–driven
% non-linearities.  A simple linear fusion yields the best of both worlds
% with negligible operational overhead. 

\subsection{Implementation Details}
\label{ssec:impl_details}

This subsection provides a detailed description of the forecasting pipeline's code structure, its main 
components, and their interactions. The implementation is organized to streamline data ingestion, 
exploratory data analysis, feature engineering, model training with cross-validation, and evaluation.

\subsubsection*{A.\ Overview and roles}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
        node distance=1.2cm and 1.6cm,
        every node/.style={font=\small, rounded corners},
        io/.style={draw, fill=blue!20, minimum height=1cm, minimum width=2.1cm, align=center},
        process/.style={draw, fill=green!20, minimum height=1cm, minimum width=2.6cm, align=center},
        module/.style={draw, fill=orange!20, minimum height=1cm, minimum width=2.6cm, align=center},
        output/.style={draw, fill=purple!20, minimum height=1cm, minimum width=2.6cm, align=center},
        file/.style={draw, fill=gray!10, minimum height=0.8cm, minimum width=2.1cm, align=center, font=\scriptsize},
        arrow/.style={->, thick}
    ]

    % Nodes
    \node[io] (config) {config.yaml\\\scriptsize settings};
    \node[process, right=of config] (runpy) {run.py\\\scriptsize orchestration};
    \node[file, below=1.2cm of runpy] (data) {dataset.csv\\\scriptsize hourly weather and energy data};
    \node[module, right=of runpy, text=red] (src) {src/\\\scriptsize eda, features, models};
    \node[output, right=of src] (out) {outputs/\\\scriptsize plots, metrics, predictions};

    % Arrows
    \draw[arrow] (config) -- (runpy);
    \draw[arrow] (data.north) -- (runpy.south);
    \draw[arrow] (runpy) -- (src);
    \draw[arrow] (src) -- (out);

    \end{tikzpicture}
    \caption{Compact overview of the forecasting pipeline.}
    \label{fig:forecast_flow_compact}
\end{figure}


\begin{itemize}
    \item \texttt{config.yaml}\\
    acts as the central knobboard, holding configuration values such as dataset path, cut-off date for the test set,
    and cross-validation horizon.
    \item \texttt{run.py}\\
    controls the workflow in seven sequential steps, from loading data to model evaluation, while logging progress 
    and writing output artefacts.
    \item \texttt{src/}\\
    contains the core logic with modules for exploratory data analysis (EDA), feature engineering, model training, 
    and evaluation metrics.
    \item \texttt{outputs/}\\
    stores generated plots (e.g., heatmaps and pairplots) and evaluation reports (metrics and JSON summaries).
\end{itemize}


\subsubsection*{B.\ Implementation}
The detailed flow inside \texttt{src/} is illustrated in Figure~\ref{fig:src_flow_final_clean}. The implementation 
distinguishes three main phases:


\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
        node distance=1.2cm and 1.2cm,
        every node/.style={font=\small, rounded corners},
        srcbox/.style={draw, fill=orange!40, minimum height=1cm, minimum width=2.6cm, align=center},
        modelbox/.style={draw, fill=orange!25, minimum height=1cm, minimum width=2.6cm, align=center},
        evalbox/.style={draw, fill=orange!5, minimum height=1cm, minimum width=2.6cm, align=center},
        outputbox/.style={draw, fill=purple!20, minimum height=1cm, minimum width=2.4cm, align=center},
        arrow/.style={->, thick},
        looplabel/.style={font=\scriptsize\itshape}
    ]

    % Main nodes
    \node[srcbox] (features) at (0, 0.6) {features.py};
    \node[srcbox, below=of features] (eda) {eda.py};
    \node[modelbox, right=1.0cm of eda, yshift=0.99cm, text=red] (models) {models/\\\scriptsize \texttt{gbdt.py}, \texttt{sarima.py}};
    \node[evalbox, right=of models] (eval) {evaluation.py};
    \node[outputbox, right=of eval] (out) {outputs/};

    % Arrows from eda/features
    \draw[arrow] (features.east) -- ++(0.7,0) |- (models.west);
    \draw[arrow] (eda.east) -- ++(0.7,0) |- (models.west);

    % Flow to evaluation/output
    \draw[arrow] (models) -- (eval);
    \draw[arrow] (eval) -- (out);

    % Feedback loop
    \draw (eval.north) -- ++(0,0.5);
    \draw[arrow] (eval.north) ++(0,0.5) -| (models);

    % Envelope around src files + evaluation
    \begin{pgfonlayer}{background}
        \node[draw=orange, thick, rounded corners, inner sep=0.4cm, 
              fit=(features) (eda) (models) (eval), 
              label=above:{\textbf{\texttt{src/} block}}] {};
    \end{pgfonlayer}

    \end{tikzpicture}
    \caption{Final horizontal \texttt{src/} flow: feature engineering and EDA enter the \texttt{models/} block, with evaluation and feedback for optimization.}
    \label{fig:src_flow_final_clean}
\end{figure}
  
\begin{itemize}
    \item \texttt{features.py}\\
    constructs engineered features including calendar-based sin/cos transforms, lagged variables, and rolling 
    means to capture temporal dependencies.
    \item \texttt{eda.py}\\
    performs quick exploratory data analysis by saving visual summaries such as correlation heatmaps and pair 
    plots for a sanity check on input data.
    \item The \texttt{models/} subfolder houses the modeling scripts (\texttt{gbdt.py} and \texttt{sarima.py}), 
    where gradient boosting and SARIMA models are trained respectively.
    \item \texttt{evaluation.py}\\
    wraps common regression metrics like MAE, RMSE, and $R^2$, and formats output tables for CLI display.
\end{itemize}

The pipeline is designed with a feedback loop from evaluation back to modeling, allowing iterative tuning or 
refinement of models based on evaluation outcomes.

\subsubsection*{C.\ Models implementation}
Figure~\ref{fig:gbdt_flow_aligned} details the flow in \texttt{gbdt.py}, the main machine learning component. 
The process starts by loading and splitting the dataset into training and validation sets. 
Importantly, instead of a standard k-fold split, a \emph{time series split} is used to preserve temporal ordering 
and avoid leakage across time in the validation procedure. This approach better reflects real-world forecasting 
challenges where future data is never accessible during training.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
        font=\small,
        node distance=1.5cm and 1.5cm,
        every node/.style={align=center, rounded corners, minimum height=1.0cm, draw},
        box/.style={fill=orange!10},
        arrow/.style={->, thick}
    ]

    % Nodes
    \node[box] (load) {Load\\ data};
    \node[box, right=of load] (split) {Split\\ data};

    \node[box, right=1.5cm of split, yshift=1.2cm] (tune) {Hyperparameter\\ tuning (GridSearch)};
    \node[box, below=1.5cm of tune] (default) {pre-set\\ params};

    \node[box, right=6cm of split] (train) {Train\\ model};

    % Arrows
    \draw[arrow] (load) -- (split);

    % Branch to tuning and default
    \draw[arrow] (split.east) -- ++(0.8,0) coordinate (branch);
    \draw[arrow] (branch) |- (tune.west);
    \draw[arrow] (branch) |- (default.west);

    % From tuning to training
    \draw[arrow] (tune.east) -- ++(0.7,0) |- (train.west);
    \draw[arrow] (default.east) -- ++(0.7,0) |- (train.west);

    \begin{pgfonlayer}{background}
        \node[draw=grey!20, thick, rounded corners, inner sep=0.2cm, 
              fit=(tune) (default), (load), (split), (train), 
              label=above:{\textcolor{gray}{\textbf{\texttt{Cross-validation ?} }}}] {};
        \node[draw=black, thick, rounded corners, inner sep=1cm, 
              fit=(tune) (default) (split) (load) (train), 
              label=below:{\textbf{\texttt{gbdt.py} }}] {};     
    \end{pgfonlayer}

    \end{tikzpicture}
    \caption{Flow of \texttt{gbdt.py}: load and split data, explore default or tuned parameters, and train the model.}
    \label{fig:gbdt_flow_aligned}
\end{figure}

The model explores two training regimes: one with pre-set default hyperparameters, and another involving an 
explicit grid search over hyperparameters to find the optimal configuration. Both approaches culminate in training 
the final model on the training data.

Cross-validation is central to the feature selection and model tuning process. Several feature subsets are 
constructed and evaluated, including:

\begin{itemize}
    \item \textbf{Manual}: A fixed set of manually chosen features.
    \item \textbf{Backward Sequential Feature Selection (BSFS)}: Starts from a large feature pool and iteratively removes less important features.
    \item \textbf{Forward Sequential Feature Selection (FwdSFS)}: Begins with a small seed set and adds features progressively.
    \item \textbf{Bayesian Optimization (BayesOpt)}: Searches over subset sizes $k$ to optimize performance.
\end{itemize}

Each subset undergoes cross-validation using the \texttt{TimeSeriesSplit} strategy with multiple splits and a defined 
forecast horizon, ensuring robust evaluation respecting the temporal structure.

Figure~\ref{fig:sarima_flow} illustrates the SARIMA pipeline flow. Unlike the gradient boosting pipeline, SARIMA 
training does not require explicit splitting of the data, as the seasonal periods are incorporated during model 
specification. A grid search over seasonal orders $(p,q,P,Q)$ identifies the best parameters based on the Akaike 
Information Criterion (AIC). After training, the model is evaluated on the test set, and results are exported for 
comparison.

Overall, the implementation balances between a fully automated pipeline and flexibility for manual adjustments and 
analysis, focusing on robust ML methods for time series forecasting.






















% In the development of our forecasting models, we encountered several unique 
% challenges that required special consideration. These challenges stemmed from 
% the nature of time-series data and the need for robust model performance. 
% Below, we highlight some of the key aspects that were addressed to ensure the 
% accuracy and reliability of our forecasts.

% \begin{adjustwidth}{1em}{0pt}
% \textbf{Cross-validation with time-series data} \\
% Cross-validation is a crucial technique for evaluating the performance of 
% time-series models. It helps us understand how well our model will perform 
% on unseen data. Classical $k$-fold—where all slices mingle chronology—with 
% our data requires us to use a \texttt{TimeSeriesSplit}, which "never looks 
% into the future".  

% Our pipeline’s 5‐fold setting keeps a fixed 720-h (1 month worth) test window in 
% each split. Figure~\ref{fig:cv} illustrates the two approaches differences. 

% \begin{figure}[h!]
% \centering
% \includegraphics[width=.46\linewidth]{images/timeseriesplit.png}
% \hfill
% \includegraphics[width=.46\linewidth]{images/kfold.png}
% \caption{Left: chronology-respecting TimeSeries CV. Right: ordinary $k$-fold mixing. 
% Ref. \cite{stackexchange_timeseries_cv}.}
% \label{fig:cv}
% \end{figure}
  
% \textbf{Correlation pruning}  
% We remove highly correlated predictors to avoid multicollinearity. It prevents 
% overfitting and speeds up the training process. While tree models sometimes may handle 
% multicollinearity, it is still a good practice to remove it and our statistical model 
% (SARIMA) requires so.
% The correlation matrix which returns a matrix pair-wise correlation values.
% It evaluates how well two variables move together along a straight line if correlated.

% \begin{lstlisting}[language=Python,basicstyle=\ttfamily\footnotesize]
% import seaborn as sns
% import matplotlib.pyplot as plt

% # Compute the correlation matrix
% corr_matrix = df.select_dtypes('number').corr()

% # Plot the heatmap
% plt.figure(figsize=(10, 8))
% sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar_kws={'label': 'Correlation Coefficient'})
% plt.title('Correlation Matrix of Predictors')
% plt.show()
% \end{lstlisting}
% The heatmap provides a visual representation of the correlation coefficients between pairs of features, aiding in the decision-making process for feature selection and pruning.

% Only numeric predictors enter the heat-map; protected columns (\texttt{y}, 
% whitelisted lags) survive even if highly correlated.

% \textbf{Selection of Predictors} \\
% Choosing the right predictors is crucial for a robust model. 
% Too many can cause overfitting, while too few may lead to underfitting, both 
% resulting in poor forecasts.

% In this pipeline, four different sets of predictors sets are evaluated to find the 
% optimal balance:
  
% \begin{figure}[h!]
%   \centering
%   \begin{tikzpicture}[node distance=14mm, box/.style={draw, rounded corners,
%                         align=center, minimum width=2.4cm, minimum height=1cm}]
%     \node[box, fill=blue!10]  (man)  {Manual\\\footnotesize 10 feats};
%     \node[box, right=of man, fill=orange!10] (bsfs) {BSFS\\\footnotesize backward SFS};
%     \node[box, right=of bsfs, fill=green!10] (fwd)  {FwdSFS\\\footnotesize forward SFS};
%     \node[box, right=of fwd, fill=red!10]    (bay)  {BayesOpt\\\footnotesize $k\!\in[3,12]$};
%   \end{tikzpicture}
%   \caption{The four feature sets evaluated.}
% \end{figure}
  
%   \textbf{Parameter selection snippet}
%   \begin{lstlisting}[language=Python,basicstyle=\ttfamily\footnotesize]
%   best_lbl = min(gbt_metrics, key=lambda k: gbt_metrics[k]['MAE'])
%   \end{lstlisting}

% \textbf{SARIMA grid \& training} \\

% First, the helper performs an exhaustive grid search
% over \(p,q\in\{0,1,2\}\) and \(P,Q\in\{0,1\}\) with seasonal period \(s=24\),
% keeping \(d=D=0\).
% The candidate that minimises AIC is retained.  

% Second, a single model is re-fit on the \emph{full} training series with that
% \((p,q,P,Q)\) order—using 
% \texttt{enforce
% \_stationarity=False} and
% \texttt{enforce\_invertibility=False}—and its predictions populate the
% 2024 hold-out window.

% \vspace{0.5em}
% \noindent\textbf{Critique}  
% The pipeline is pragmatic yet minimal: it skips data-type validation, missing-value imputation, 
% and GPU-accelerated boosters—which may matter on noisier or larger grids.  Still, for a clean 
% renewables feed the current structure delivers repeatable, inspectable forecasts with zero hidden 
% state.


% \label{subsec:impl}

% \begin{enumerate}
% \item \textbf{Repository layout.}  
%       \texttt{src/}\{data\_io,features,models,eval\} mirror the
%       methodology; notebooks live under \texttt{notebooks/}.
% \item \textbf{Data pipeline.}  
%       \texttt{data\_io.py} cleans $>10$M rows in a
%       streaming fashion, then \texttt{FeatureEngineer} creates the
%       37-column table and serialises scalers with
%       \texttt{joblib} for reuse in prod.
% \item \textbf{Time-series CV.}  
%       Rolling splits (\texttt{RollingOriginEvaluator}) preserve causality:
%       train $\to$ validate windows grow by 24h each step.
% \item \textbf{Hyper-parameter tuning.}  
%       \emph{Bayesian optimisation} (\texttt{optuna}) over
%       \{\#trees, depth, $\eta$, col\_sample, $\lambda$\} maximises
%       negative validation MAE; 50 trials finish in 3.2h on CPU.
% \item \textbf{Training.}  
%       With tuned params the full train+val set (2014–2023) is fit; model
%       size $\approx$ 3.6MB.
% \item \textbf{Inference \& evaluation.}  
%       A single \texttt{predict\_day.py} script generates a
%       24-step forecast, plots four-panel diagnostics and writes
%       JSON +\;PNG to \texttt{reports/daily\_tests/}.
% \item \textbf{MLOps hooks.}  
%       GitHub Actions run the daily notebook on the latest data dump;
%       threshold alarms fire on MAE drift $>10\%$ vs rolling median.
% \end{enumerate}

% \subsection{Key Take-Aways}
% \label{subsec:key-takeaways}

% \begin{itemize}
%   \item \textbf{Seasonality explains most variance.}
%         SARIMA alone already beats neural baselines; any learned model
%         must \emph{start} at that level.
%   \item \textbf{Non-linear weather interactions matter.}
%         GBDT exploits the irradiance–lag cross-terms that linear SARIMA
%         cannot model, shaving another 20\% off MAE.
%   \item \textbf{Hybrid stacking is cheap and wins.}  
%         Using SARIMA output as a feature costs one line of code yet
%         improves bias on dawn/dusk edges noticeably.
%   \item \textbf{EDAs saved us weeks.}  
%         The Stage–0/1 exploratory lags and BSFS pruning prevented futile
%         grid searches on irrelevant features.
%   \item \textbf{Trees > deep nets for tabular.}  
%         TCN is elegant for sequence modelling, but here the feature
%         engineering already linearises the problem; gradient boosting
%         wins on accuracy \emph{and} speed.
% \end{itemize}

% The resulting solution is reproducible end-to-end, MLOps compliant and
% ready for deployment on the plant’s SCADA edge server.
