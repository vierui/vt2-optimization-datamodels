{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# ML / DL\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, callbacks\n",
    "import keras_tuner as kt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run started  2025-06-14 13:41:42.586647\n"
     ]
    }
   ],
   "source": [
    "LAGS          = list(range(1, 25))         # past 24 hours\n",
    "STEP          = \"H\"\n",
    "MAX_EPOCHS    = 50                         # <<< reduced\n",
    "EARLY_STOP    = 5\n",
    "TRIALS        = 20\n",
    "EXEC_PER_TR   = 2\n",
    "BATCH_SIZE    = 64\n",
    "KEY_DATES     = [\"2024-01-01\", \"2024-02-01\", \"2024-03-01\", \"2024-04-01\"]\n",
    "\n",
    "# print(f\"Run started  {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pd.read_csv(\"../../data/renewables/pv_with_weather_data.csv\",\n",
    "                comment=\"#\", parse_dates=[\"time\"])\n",
    "    .rename(columns={\"time\": \"ts\",\n",
    "                     \"electricity\": \"pv\",\n",
    "                     \"irradiance_direct\": \"dir_irr\",\n",
    "                     \"irradiance_diffuse\": \"dif_irr\",\n",
    "                     \"temperature\": \"temp\"})\n",
    "    .set_index(\"ts\")\n",
    "    .asfreq(STEP)\n",
    ")\n",
    "print(f\"Data span : {df.index.min()} – {df.index.max()} ({len(df):,} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(data, lags, include_time=True, include_weather=True):\n",
    "    X = pd.DataFrame(index=data.index)\n",
    "    for lag in lags:\n",
    "        X[f\"lag_{lag}\"] = data[\"pv\"].shift(lag)\n",
    "\n",
    "    if include_time:\n",
    "        hr = data.index.hour\n",
    "        dy = data.index.dayofyear\n",
    "        X[\"sin_hour\"] = np.sin(2 * np.pi * hr / 24)\n",
    "        X[\"cos_hour\"] = np.cos(2 * np.pi * hr / 24)\n",
    "        X[\"sin_doy\"]  = np.sin(2 * np.pi * dy / 365)\n",
    "        X[\"cos_doy\"]  = np.cos(2 * np.pi * dy / 365)\n",
    "    if include_weather:\n",
    "        X[\"dir_irr\"] = data[\"dir_irr\"]\n",
    "        X[\"dif_irr\"] = data[\"dif_irr\"]\n",
    "        X[\"temp\"]    = data[\"temp\"]\n",
    "\n",
    "    y = data[\"pv\"]\n",
    "    return X.join(y.rename(\"target\")).dropna()\n",
    "\n",
    "feats_all = make_features(df, LAGS)\n",
    "print(\"Feature matrix shape :\", feats_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3. SPLITS\n",
    "# ------------------------------------------------------------\n",
    "train_val_end = \"2023-12-31 23:00\"\n",
    "holdout_start = \"2024-01-01 00:00\"\n",
    "\n",
    "train_val = feats_all.loc[:train_val_end]\n",
    "hold_out  = feats_all.loc[holdout_start:]\n",
    "\n",
    "# internal chronological 80/20 split\n",
    "split_idx = int(len(train_val) * 0.8)\n",
    "train = train_val.iloc[:split_idx]\n",
    "ival  = train_val.iloc[split_idx:]\n",
    "\n",
    "def split_xy(frame):\n",
    "    X = frame.drop(columns=\"target\").values\n",
    "    y = frame[\"target\"].values\n",
    "    return X, y\n",
    "\n",
    "Xt_tr, yt_tr = split_xy(train)\n",
    "Xt_val, yt_val = split_xy(ival)\n",
    "Xt_hold, yt_hold = split_xy(hold_out)\n",
    "\n",
    "# scalers\n",
    "x_scaler = StandardScaler().fit(Xt_tr)\n",
    "y_scaler = StandardScaler().fit(yt_tr.reshape(-1, 1))\n",
    "\n",
    "def scale_x(X): return x_scaler.transform(X)\n",
    "def unscale_y(y): return y_scaler.inverse_transform(y.reshape(-1, 1)).ravel()\n",
    "\n",
    "Xt_tr_s, Xt_val_s = scale_x(Xt_tr), scale_x(Xt_val)\n",
    "Xt_hold_s          = scale_x(Xt_hold)\n",
    "yt_tr_s            = y_scaler.transform(yt_tr.reshape(-1, 1)).ravel()\n",
    "\n",
    "print(f\"Train: {len(train):,}  | Val: {len(ival):,} | Hold-out: {len(hold_out):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_model(hp):\n",
    "    n_feat = Xt_tr_s.shape[1]\n",
    "\n",
    "    n_layers = hp.Int(\"n_layers\", 1, 3)\n",
    "    units    = hp.Choice(\"units\", [32, 64, 128])\n",
    "    drop     = hp.Float(\"dropout\", 0.0, 0.4, step=0.1)\n",
    "    l2_reg   = hp.Float(\"l2\", 1e-5, 1e-2, sampling=\"log\")\n",
    "    lr       = hp.Choice(\"lr\", [1e-3, 3e-4, 1e-4])\n",
    "\n",
    "    m = models.Sequential()\n",
    "    m.add(layers.Input(shape=(n_feat,)))\n",
    "    for _ in range(n_layers):\n",
    "        m.add(layers.Dense(units, activation=\"relu\",\n",
    "                           kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "        m.add(layers.Dropout(drop))\n",
    "\n",
    "    m.add(layers.Dense(1))\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
    "              loss=\"mse\", metrics=[\"mae\"])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    hp_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=TRIALS,\n",
    "    executions_per_trial=EXEC_PER_TR,\n",
    "    directory=\"tuner_dir\",\n",
    "    project_name=\"pv_nn\"\n",
    ")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "es_cb = callbacks.EarlyStopping(patience=EARLY_STOP, restore_best_weights=True)\n",
    "\n",
    "tuner.search(\n",
    "    Xt_tr_s, yt_tr_s,\n",
    "    validation_data=(Xt_val_s, y_scaler.transform(yt_val.reshape(-1,1))),\n",
    "    epochs=MAX_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[es_cb],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"\\nBest HPs:\", best_hp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best hyperparameters found from previous tuning\n",
    "best_hp_values = {'n_layers': 1, 'units': 128, 'dropout': 0.0, 'l2': 0.003479689743431989, 'lr': 0.0003}\n",
    "print(\"\\nUsing Best HPs:\", best_hp_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrain on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = tuner.hypermodel.build(best_hp)\n",
    "# full_X = np.concatenate([Xt_tr_s, Xt_val_s])\n",
    "# full_y = np.concatenate([yt_tr_s,\n",
    "#                          y_scaler.transform(yt_val.reshape(-1,1)).ravel()])\n",
    "\n",
    "# history = best_model.fit(\n",
    "#     full_X, full_y,\n",
    "#     epochs=MAX_EPOCHS,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     callbacks=[es_cb],\n",
    "#     verbose=0\n",
    "# )\n",
    "\n",
    "# Create the best model directly using the found hyperparameters\n",
    "def create_best_model(hp_values):\n",
    "    n_feat = Xt_tr_s.shape[1]\n",
    "    \n",
    "    m = models.Sequential()\n",
    "    m.add(layers.Input(shape=(n_feat,)))\n",
    "    for _ in range(hp_values['n_layers']):\n",
    "        m.add(layers.Dense(hp_values['units'], activation=\"relu\",\n",
    "                           kernel_regularizer=regularizers.l2(hp_values['l2'])))\n",
    "        m.add(layers.Dropout(hp_values['dropout']))\n",
    "    \n",
    "    m.add(layers.Dense(1))\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(hp_values['lr']),\n",
    "              loss=\"mse\", metrics=[\"mae\"])\n",
    "    return m\n",
    "\n",
    "best_model = create_best_model(best_hp_values)\n",
    "full_X = np.concatenate([Xt_tr_s, Xt_val_s])\n",
    "full_y = np.concatenate([yt_tr_s,\n",
    "                         y_scaler.transform(yt_val.reshape(-1,1)).ravel()])\n",
    "\n",
    "es_cb = callbacks.EarlyStopping(patience=EARLY_STOP, restore_best_weights=True)\n",
    "history = best_model.fit(\n",
    "    full_X, full_y,\n",
    "    epochs=MAX_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[es_cb],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(m, Xs, y_true):\n",
    "    y_pred_s = m.predict(Xs, verbose=0).flatten()\n",
    "    y_pred   = unscale_y(y_pred_s)\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return mae, rmse, y_pred\n",
    "\n",
    "mae_hold, rmse_hold, y_hat_hold = evaluate(best_model, Xt_hold_s, yt_hold)\n",
    "print(f\"\\nHold-out 2024  |  MAE={mae_hold:.3f}  RMSE={rmse_hold:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## daily mae for requested dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDaily MAE on selected dates (hold-out):\")\n",
    "hold_df = pd.DataFrame({\n",
    "    \"actual\": yt_hold,\n",
    "    \"pred\"  : y_hat_hold\n",
    "}, index=hold_out.index)\n",
    "\n",
    "for d in KEY_DATES:\n",
    "    day = pd.to_datetime(d)\n",
    "    mask = hold_df.index.normalize() == day\n",
    "    if mask.any():\n",
    "        mae_day = np.mean(np.abs(hold_df.loc[mask, \"actual\"] -\n",
    "                                 hold_df.loc[mask, \"pred\"]))\n",
    "        print(f\"{d}: {mae_day:.3f}\")\n",
    "    else:\n",
    "        print(f\"{d}: not in hold-out set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating hourly prediction plots for key dates...\")\n",
    "\n",
    "# Create subplots for each key date\n",
    "fig, axes = plt.subplots(len(KEY_DATES), 1, figsize=(12, 4*len(KEY_DATES)))\n",
    "if len(KEY_DATES) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, date_str in enumerate(KEY_DATES):\n",
    "    day = pd.to_datetime(date_str)\n",
    "    mask = hold_df.index.normalize() == day\n",
    "    \n",
    "    if mask.any():\n",
    "        day_data = hold_df.loc[mask].copy()\n",
    "        hours = day_data.index.hour\n",
    "        \n",
    "        axes[i].plot(hours, day_data[\"actual\"], 'o-', label='Actual', \n",
    "                    color='blue', linewidth=2, markersize=4)\n",
    "        axes[i].plot(hours, day_data[\"pred\"], 's-', label='Predicted', \n",
    "                    color='red', linewidth=2, markersize=4, alpha=0.8)\n",
    "        \n",
    "        axes[i].set_title(f'Hourly PV Forecast vs Actual - {date_str}', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel('Hour of Day')\n",
    "        axes[i].set_ylabel('PV Generation')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        axes[i].set_xticks(range(0, 24, 2))\n",
    "        \n",
    "        # Add MAE annotation\n",
    "        mae_day = np.mean(np.abs(day_data[\"actual\"] - day_data[\"pred\"]))\n",
    "        axes[i].text(0.02, 0.98, f'MAE: {mae_day:.4f}', \n",
    "                    transform=axes[i].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    else:\n",
    "        axes[i].text(0.5, 0.5, f'{date_str}: No data available', \n",
    "                    ha='center', va='center', transform=axes[i].transAxes)\n",
    "        axes[i].set_title(f'Data not available - {date_str}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a combined plot showing all key dates together\n",
    "plt.figure(figsize=(15, 6))\n",
    "colors = ['blue', 'green', 'purple', 'orange', 'red', 'brown', 'pink', 'gray']\n",
    "markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p']\n",
    "\n",
    "# Ensure we have enough colors and markers for all dates\n",
    "num_dates = len(KEY_DATES)\n",
    "if num_dates > len(colors):\n",
    "    colors = colors * ((num_dates // len(colors)) + 1)\n",
    "if num_dates > len(markers):\n",
    "    markers = markers * ((num_dates // len(markers)) + 1)\n",
    "\n",
    "for i, date_str in enumerate(KEY_DATES):\n",
    "    day = pd.to_datetime(date_str)\n",
    "    mask = hold_df.index.normalize() == day\n",
    "    \n",
    "    if mask.any():\n",
    "        day_data = hold_df.loc[mask].copy()\n",
    "        hours = day_data.index.hour\n",
    "        \n",
    "        # Actual data\n",
    "        plt.plot(hours + i*0.1, day_data[\"actual\"], \n",
    "                marker=markers[i], linestyle='-', \n",
    "                color=colors[i], label=f'Actual {date_str}', \n",
    "                linewidth=2, markersize=6, alpha=0.8)\n",
    "        \n",
    "        # Predicted data  \n",
    "        plt.plot(hours + i*0.1, day_data[\"pred\"], \n",
    "                marker=markers[i], linestyle='--', \n",
    "                color=colors[i], label=f'Predicted {date_str}', \n",
    "                linewidth=2, markersize=4, alpha=0.6)\n",
    "\n",
    "plt.title('Hourly PV Forecast vs Actual - All Key Dates Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('PV Generation')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(0, 24, 2))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.title(\"Training loss (full 2015–2023 retrain)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hp landscape look\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model architecture:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_hp_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# trials = []\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# for t in tuner.oracle.get_trials():\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     rec = t.hyperparameters.values.copy()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# plt.ylabel(\"n_layers\"); plt.xlabel(\"units\")\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest model architecture:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- Layers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbest_hp_values\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_layers\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- Units: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_hp_values[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- Dropout: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_hp_values[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_hp_values' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# trials = []\n",
    "# for t in tuner.oracle.get_trials():\n",
    "#     rec = t.hyperparameters.values.copy()\n",
    "#     rec[\"val_loss\"] = t.score\n",
    "#     trials.append(rec)\n",
    "# hp_df = pd.DataFrame(trials)\n",
    "\n",
    "# sns.pairplot(hp_df, vars=[c for c in hp_df.columns if c!=\"val_loss\"],\n",
    "#              hue=\"val_loss\", palette=\"viridis\")\n",
    "# plt.suptitle(\"HP pair-plot (colour = val_loss)\", y=1.02)\n",
    "# plt.show()\n",
    "\n",
    "# # heat-map layers × units\n",
    "# pivot = (hp_df\n",
    "#          .groupby([\"n_layers\",\"units\"], as_index=False)[\"val_loss\"]\n",
    "#          .mean()\n",
    "#          .pivot(index=\"n_layers\", columns=\"units\", values=\"val_loss\"))\n",
    "# sns.heatmap(pivot, annot=True, fmt=\".4f\", cmap=\"YlGnBu\", cbar_kws={'label':'mean val_loss'})\n",
    "# plt.title(\"Validation loss – layers × units\")\n",
    "# plt.ylabel(\"n_layers\"); plt.xlabel(\"units\")\n",
    "# plt.show()\n",
    "\n",
    "print(f\"\\nBest model architecture:\")\n",
    "print(f\"- Layers: {best_hp_values['n_layers']}\")\n",
    "print(f\"- Units: {best_hp_values['units']}\")  \n",
    "print(f\"- Dropout: {best_hp_values['dropout']}\")\n",
    "print(f\"- L2 regularization: {best_hp_values['l2']:.6f}\")\n",
    "print(f\"- Learning rate: {best_hp_values['lr']}\")\n",
    "print(f\"\\nRun completed {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vt2-optimization-datamodels-NPqtCblQ-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
