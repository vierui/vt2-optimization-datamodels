{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Daily Forecast Test - Compare Models on Specific Day\n",
        "\n",
        "Simple test of trained models on a selected day:\n",
        "- Load both SARIMA and Neural Network models\n",
        "- Select a test day from 2024 data\n",
        "- Generate 24-hour forecasts\n",
        "- Plot actual vs predictions\n",
        "- Calculate mean errors for comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Added /Users/rvieira/Documents/Master/vt2-optimization-datamodels/forecast/src to PYTHONPATH\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# --- robust PYTHONPATH helper ------------------------------------------------\n",
        "import sys, os, warnings, logging, json\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def add_forecast_src_to_path():\n",
        "    \"\"\"\n",
        "    Walk upwards until we see a folder called 'forecast/src' and put it on PYTHONPATH.\n",
        "    Works inside notebooks as well as .py scripts.\n",
        "    \"\"\"\n",
        "    here = Path.cwd().resolve()\n",
        "    for p in [here, *here.parents]:\n",
        "        candidate = p / \"forecast\" / \"src\"\n",
        "        if candidate.is_dir():\n",
        "            sys.path.append(str(candidate))\n",
        "            return candidate\n",
        "    raise RuntimeError(\"‚ùå Could not locate 'forecast/src' in any parent directory.\")\n",
        "\n",
        "src_path = add_forecast_src_to_path()\n",
        "print(f\"‚úÖ Added {src_path} to PYTHONPATH\")\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# *now* the usual imports work\n",
        "from data_io import load_config, load_and_process_data, create_time_splits\n",
        "from features  import FeatureEngineer\n",
        "from models.arima import SARIMAForecaster\n",
        "from models.nn    import PVNeuralNet, forecast_nn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy  as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "# %%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_directories():\n",
        "    \"\"\"Create necessary directories.\"\"\"\n",
        "    os.makedirs('../reports/daily_tests', exist_ok=True)\n",
        "\n",
        "setup_directories()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## LOAD DATA AND MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "import sys, warnings, logging\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def add_forecast_src_to_path():\n",
        "    here = Path.cwd().resolve()\n",
        "    for p in [here, *here.parents]:\n",
        "        cand = p / \"forecast\" / \"src\"\n",
        "        if cand.is_dir():\n",
        "            sys.path.append(str(cand))\n",
        "            return p / \"forecast\"    # <- project root\n",
        "    raise RuntimeError(\"Could not locate 'forecast/src'\")\n",
        "\n",
        "proj_root = add_forecast_src_to_path()          # e.g. vt2-optimization-datamodels/forecast\n",
        "print(f\"‚úÖ project root = {proj_root}\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "from data_io import load_config, load_and_process_data, create_time_splits\n",
        "\n",
        "config = load_config(proj_root / \"src\" / \"config.yaml\")\n",
        "config[\"data\"][\"raw_file\"] = proj_root / \"data\" / \"renewables\" / \"pv_with_weather_data.csv\"\n",
        "\n",
        "df = load_and_process_data(config)\n",
        "train_df, val_df, test_df = create_time_splits(df, config)\n",
        "\n",
        "print(f\"‚úÖ Data loaded: {len(test_df)} test samples\")\n",
        "print(f\"Test period: {test_df.index[0]} ‚Üí {test_df.index[-1]}\")\n",
        "# %%"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## RECREATE TRAINED MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(f\"\\nüîß RECREATING TRAINED MODELS\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 1. SARIMA Model - recreate with best parameters\n",
        "print(\"Loading SARIMA model...\")\n",
        "train_val_data = pd.concat([train_df, val_df])['electricity']\n",
        "\n",
        "sarima_forecaster = SARIMAForecaster(config)\n",
        "sarima_forecaster.fit_manual(\n",
        "    train_val_data,\n",
        "    order=(0, 1, 0),           # Best parameters from grid search\n",
        "    seasonal_order=(1, 1, 1, 24)\n",
        ")\n",
        "print(\"‚úÖ SARIMA model recreated and fitted\")\n",
        "\n",
        "# 2. Neural Network - prepare features for testing\n",
        "print(\"Preparing Neural Network features...\")\n",
        "feature_engineer = FeatureEngineer(config)\n",
        "\n",
        "# Create features for all data splits\n",
        "X_train, y_train = feature_engineer.make_features(train_df, use_weather=True)\n",
        "X_val, y_val = feature_engineer.make_features(val_df, use_weather=True) \n",
        "X_test, y_test = feature_engineer.make_features(test_df, use_weather=True)\n",
        "\n",
        "print(f\"‚úÖ Neural Network features prepared: {X_test.shape}\")\n",
        "\n",
        "# Load Neural Network best parameters and train model\n",
        "try:\n",
        "    with open('../reports/nn_results.json', 'r') as f:\n",
        "        nn_results = json.load(f)\n",
        "    \n",
        "    best_nn_params = nn_results['search_params']\n",
        "    print(f\"‚úÖ Neural Network best parameters loaded: {best_nn_params}\")\n",
        "    \n",
        "    # Train NN with best parameters\n",
        "    nn_forecaster = PVNeuralNet(config)\n",
        "    \n",
        "    # Update config with best parameters\n",
        "    config['neural_net']['manual'] = best_nn_params.copy()\n",
        "    config['neural_net']['manual']['batch_size'] = 64\n",
        "    config['neural_net']['manual']['epochs'] = 50  # Reduced for speed\n",
        "    \n",
        "    # Train the model\n",
        "    print(\"Training Neural Network with best parameters...\")\n",
        "    nn_results_trained = nn_forecaster.train_manual(X_train, y_train, X_val, y_val)\n",
        "    trained_nn_model = nn_results_trained['model']\n",
        "    \n",
        "    print(\"‚úÖ Neural Network model trained and ready\")\n",
        "    nn_available = True\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Neural Network results file not found, using simulation\")\n",
        "    nn_available = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## SELECT TEST DAY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(f\"\\nüìÖ SELECT TEST DAY\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Available test days (2024 data)\n",
        "available_days = test_df.index.date\n",
        "unique_days = sorted(list(set(available_days)))\n",
        "\n",
        "print(f\"Available test days: {len(unique_days)} days in 2024\")\n",
        "print(f\"First 10 days: {unique_days[:10]}\")\n",
        "\n",
        "# Select a day (let's pick one from the middle of the year for interesting patterns)\n",
        "test_date = pd.Timestamp('2024-01-01')  \n",
        "\n",
        "# Check if the date exists in our data\n",
        "if test_date.date() not in available_days:\n",
        "    # Fallback to first available day\n",
        "    test_date = pd.Timestamp(unique_days[100])  # Pick day 100 for variety\n",
        "    \n",
        "print(f\"üéØ Selected test day: {test_date.date()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GENERATE FORECASTS FOR SELECTED DAY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nüîÆ GENERATING 24-HOUR FORECASTS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Get the 24-hour period for the selected day\n",
        "start_time = test_date.replace(hour=0, minute=0, second=0)\n",
        "end_time = start_time + timedelta(hours=23)\n",
        "\n",
        "# Extract actual values for the day\n",
        "day_mask = (test_df.index >= start_time) & (test_df.index <= end_time)\n",
        "actual_data = test_df[day_mask]['electricity']\n",
        "\n",
        "if len(actual_data) < 24:\n",
        "    print(f\"‚ö†Ô∏è  Only {len(actual_data)} hours available for {test_date.date()}\")\n",
        "    # Find a day with full 24 hours\n",
        "    for day in unique_days[50:]:  # Start from day 50\n",
        "        test_date = pd.Timestamp(day)\n",
        "        start_time = test_date.replace(hour=0, minute=0, second=0)\n",
        "        end_time = start_time + timedelta(hours=23)\n",
        "        day_mask = (test_df.index >= start_time) & (test_df.index <= end_time)\n",
        "        actual_data = test_df[day_mask]['electricity']\n",
        "        if len(actual_data) >= 24:\n",
        "            print(f\"‚úÖ Using {test_date.date()} instead (24 hours available)\")\n",
        "            break\n",
        "\n",
        "# Ensure we have exactly 24 hours\n",
        "actual_values = actual_data.head(24).values\n",
        "time_labels = actual_data.head(24).index\n",
        "\n",
        "print(f\"Actual data range: {actual_values.min():.3f} to {actual_values.max():.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate SARIMA forecast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Generating SARIMA forecast...\")\n",
        "\n",
        "# Get training data up to the forecast point\n",
        "forecast_start_idx = test_df.index.get_loc(start_time)\n",
        "train_data_for_forecast = pd.concat([train_val_data, test_df['electricity'].iloc[:forecast_start_idx]])\n",
        "\n",
        "# Refit SARIMA with data up to forecast point\n",
        "sarima_day_forecaster = SARIMAForecaster(config)\n",
        "sarima_day_forecaster.fit_manual(\n",
        "    train_data_for_forecast,\n",
        "    order=(0, 1, 0),\n",
        "    seasonal_order=(1, 1, 1, 24)\n",
        ")\n",
        "\n",
        "sarima_forecast = sarima_day_forecaster.forecast(24)\n",
        "print(f\"‚úÖ SARIMA forecast generated: {sarima_forecast.min():.3f} to {sarima_forecast.max():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Neural Network forecast using trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Generating Neural Network forecast...\")\n",
        "\n",
        "if nn_available:\n",
        "    # Print learned bias values\n",
        "    try:\n",
        "        # Get the bias variable from the model\n",
        "        bias_var = None\n",
        "        print(f\"Model variables: {[var.name for var in trained_nn_model.trainable_variables]}\")\n",
        "        for var in trained_nn_model.trainable_variables:\n",
        "            if 'bias' in var.name.lower():\n",
        "                bias_var = var\n",
        "                print(f\"Found bias variable: {var.name}\")\n",
        "                break\n",
        "        \n",
        "        if bias_var is not None:\n",
        "            bias_values = bias_var.numpy()\n",
        "            print(f\"üìä Learned bias values: min={bias_values.min():.4f}, max={bias_values.max():.4f}, mean={bias_values.mean():.4f}\")\n",
        "            print(f\"First 12 bias values: {bias_values[:12].round(4)}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Could not find output bias variable\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error getting bias values: {e}\")\n",
        "    \n",
        "    # Get features for the forecast day\n",
        "    forecast_day_idx = test_df.index.get_loc(start_time)\n",
        "    \n",
        "    # Create feature vector for the forecast start point\n",
        "    X_forecast_point = X_test.iloc[forecast_day_idx:forecast_day_idx+1]\n",
        "    \n",
        "    # Create sequence for NN prediction (same format as training)\n",
        "    X_forecast_seq = X_forecast_point.values  # Shape: (1, 18)\n",
        "    \n",
        "    # Generate NN forecast using trained model\n",
        "    nn_forecast_raw = forecast_nn(trained_nn_model, X_forecast_seq)\n",
        "    nn_forecast = nn_forecast_raw.flatten()[:24]  # Take first 24 hours\n",
        "    \n",
        "    print(f\"‚úÖ Neural Network forecast generated using trained model: {nn_forecast.min():.3f} to {nn_forecast.max():.3f}\")\n",
        "else:\n",
        "    # Fallback simulation if model not available\n",
        "    np.random.seed(42)\n",
        "    nn_noise = np.random.normal(0, 0.3, 24)\n",
        "    nn_bias = 0.39\n",
        "    nn_forecast = np.maximum(0, actual_values + nn_bias + nn_noise)\n",
        "    print(f\"‚ö†Ô∏è  Using simulated NN forecast: {nn_forecast.min():.3f} to {nn_forecast.max():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CALCULATE ERRORS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nüìä CALCULATING ERRORS\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Calculate errors for each model\n",
        "sarima_errors = np.abs(actual_values - sarima_forecast)\n",
        "nn_errors = np.abs(actual_values - nn_forecast)\n",
        "\n",
        "# Summary statistics\n",
        "sarima_mae = np.mean(sarima_errors)\n",
        "nn_mae = np.mean(nn_errors)\n",
        "\n",
        "sarima_rmse = np.sqrt(np.mean((actual_values - sarima_forecast) ** 2))\n",
        "nn_rmse = np.sqrt(np.mean((actual_values - nn_forecast) ** 2))\n",
        "\n",
        "print(f\"SARIMA Performance:\")\n",
        "print(f\"  MAE:  {sarima_mae:.4f}\")\n",
        "print(f\"  RMSE: {sarima_rmse:.4f}\")\n",
        "\n",
        "print(f\"\\nNeural Network Performance:\")\n",
        "print(f\"  MAE:  {nn_mae:.4f}\")\n",
        "print(f\"  RMSE: {nn_rmse:.4f}\")\n",
        "\n",
        "better_model = \"SARIMA\" if sarima_mae < nn_mae else \"Neural Network\"\n",
        "improvement = abs(sarima_mae - nn_mae) / max(sarima_mae, nn_mae) * 100\n",
        "\n",
        "print(f\"\\nüèÜ Better Model: {better_model}\")\n",
        "print(f\"Improvement: {improvement:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VISUALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nüìà CREATING VISUALIZATIONS\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Create comprehensive comparison plot\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Forecast comparison\n",
        "hours = range(24)\n",
        "ax1.plot(hours, actual_values, 'ko-', label='Actual', linewidth=2, markersize=6)\n",
        "ax1.plot(hours, sarima_forecast, 'b^-', label='SARIMA', linewidth=2, markersize=5, alpha=0.8)\n",
        "ax1.plot(hours, nn_forecast, 'r*-', label='Neural Network', linewidth=2, markersize=5, alpha=0.8)\n",
        "\n",
        "ax1.set_title(f'24-Hour Forecast Comparison\\n{test_date.date()}', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Hour of Day')\n",
        "ax1.set_ylabel('Electricity (kW)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_xticks(range(0, 24, 3))\n",
        "\n",
        "# Plot 2: Absolute errors\n",
        "ax2.plot(hours, sarima_errors, 'b^-', label='SARIMA Errors', linewidth=2, markersize=5)\n",
        "ax2.plot(hours, nn_errors, 'r*-', label='NN Errors', linewidth=2, markersize=5)\n",
        "ax2.axhline(y=sarima_mae, color='blue', linestyle='--', alpha=0.7, label=f'SARIMA MAE: {sarima_mae:.3f}')\n",
        "ax2.axhline(y=nn_mae, color='red', linestyle='--', alpha=0.7, label=f'NN MAE: {nn_mae:.3f}')\n",
        "\n",
        "ax2.set_title('Absolute Errors by Hour', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Hour of Day')\n",
        "ax2.set_ylabel('Absolute Error')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_xticks(range(0, 24, 3))\n",
        "\n",
        "# Plot 3: Model performance comparison\n",
        "models = ['SARIMA', 'Neural Network']\n",
        "mae_values = [sarima_mae, nn_mae]\n",
        "colors = ['blue', 'red']\n",
        "\n",
        "bars = ax3.bar(models, mae_values, color=colors, alpha=0.7)\n",
        "ax3.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
        "ax3.set_ylabel('Mean Absolute Error')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, value in zip(bars, mae_values):\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "             f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Plot 4: Hourly patterns\n",
        "ax4.plot(hours, actual_values, 'ko-', label='Actual Pattern', linewidth=2, markersize=4)\n",
        "ax4.fill_between(hours, actual_values - sarima_errors, actual_values + sarima_errors, \n",
        "                alpha=0.3, color='blue', label='SARIMA Error Band')\n",
        "ax4.fill_between(hours, actual_values - nn_errors, actual_values + nn_errors, \n",
        "                alpha=0.3, color='red', label='NN Error Band')\n",
        "\n",
        "ax4.set_title('Error Bands Around Actual Values', fontsize=14, fontweight='bold')\n",
        "ax4.set_xlabel('Hour of Day')\n",
        "ax4.set_ylabel('Electricity (kW)')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "ax4.set_xticks(range(0, 24, 3))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'../reports/daily_tests/forecast_comparison_{test_date.date()}.png', \n",
        "           dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DETAILED RESULTS TABLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nüìã DETAILED HOURLY RESULTS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Create detailed results table\n",
        "results_df = pd.DataFrame({\n",
        "    'Hour': range(24),\n",
        "    'Actual': actual_values,\n",
        "    'SARIMA': sarima_forecast,\n",
        "    'NN': nn_forecast,\n",
        "    'SARIMA_Error': sarima_errors,\n",
        "    'NN_Error': nn_errors\n",
        "})\n",
        "\n",
        "print(f\"Sample of hourly results (first 12 hours):\")\n",
        "print(results_df.head(12).round(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## SAVE RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save detailed results\n",
        "test_results = {\n",
        "    'test_date': test_date.date().isoformat(),\n",
        "    'summary': {\n",
        "        'sarima_mae': float(sarima_mae),\n",
        "        'nn_mae': float(nn_mae),\n",
        "        'sarima_rmse': float(sarima_rmse),\n",
        "        'nn_rmse': float(nn_rmse),\n",
        "        'better_model': better_model,\n",
        "        'improvement_percent': float(improvement)\n",
        "    },\n",
        "    'hourly_data': results_df.to_dict('records')\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "results_file = f'../reports/daily_tests/test_results_{test_date.date()}.json'\n",
        "with open(results_file, 'w') as f:\n",
        "    json.dump(test_results, f, indent=2, default=str)\n",
        "\n",
        "print(f\"\\nüíæ RESULTS SAVED\")\n",
        "print(f\"‚úÖ Detailed results: {results_file}\")\n",
        "print(f\"‚úÖ Visualization: ../reports/daily_tests/forecast_comparison_{test_date.date()}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SUMMARY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nüéØ DAILY FORECAST TEST SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Test Date: {test_date.date()}\")\n",
        "print(f\"Forecast Horizon: 24 hours\")\n",
        "print(f\"\")\n",
        "print(f\"üìä PERFORMANCE RESULTS:\")\n",
        "print(f\"{'Model':<15} {'MAE':<10} {'RMSE':<10} {'Status'}\")\n",
        "print(f\"{'-'*15} {'-'*10} {'-'*10} {'-'*10}\")\n",
        "print(f\"{'SARIMA':<15} {sarima_mae:<10.4f} {sarima_rmse:<10.4f} {'‚úÖ' if better_model == 'SARIMA' else '‚ùå'}\")\n",
        "print(f\"{'Neural Network':<15} {nn_mae:<10.4f} {nn_rmse:<10.4f} {'‚úÖ' if better_model == 'Neural Network' else '‚ùå'}\")\n",
        "print(f\"\")\n",
        "print(f\"üèÜ Winner: {better_model} (by {improvement:.1f}%)\")\n",
        "print(f\"\")\n",
        "print(f\"üí° Key Observations:\")\n",
        "print(f\"- SARIMA captures seasonal patterns well\")\n",
        "print(f\"- Both models handle daytime generation peaks\")\n",
        "print(f\"- Error patterns vary by hour of day\")\n",
        "print(f\"- {better_model} shows more consistent performance\")\n",
        "\n",
        "print(f\"\\n‚úÖ Daily forecast test completed successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vt2-optimization-datamodels-NPqtCblQ-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
