{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# PV Forecasting with Error Propagation Analysis\n",
        "\n",
        "1. **Parameter Range Search**: Find optimal hyperparameters\n",
        "2. **Random Date Evaluation**: Analyze model performance on random dates\n",
        "3. **Error Propagation Analysis**: Track how errors evolve throughout 2024\n",
        "4. **Daily Error Metrics**: Comprehensive daily error analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, time, warnings, random\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# ML / DL\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers, callbacks\n",
        "import keras_tuner as kt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run started: 2025-06-18 20:07:15.965530\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "LAGS          = list(range(1, 25))         # past 24 hours\n",
        "STEP          = \"H\"\n",
        "MAX_EPOCHS    = 50\n",
        "EARLY_STOP    = 5\n",
        "TRIALS        = 30                         # Increased for better search\n",
        "EXEC_PER_TR   = 2\n",
        "BATCH_SIZE    = 64\n",
        "\n",
        "print(f\"Run started: {datetime.now()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Data Loading & Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data span: 2014-01-01 00:00:00 – 2024-12-31 23:00:00 (96,432 rows)\n",
            "Training period: 2014-01-01 00:00:00 to 2023-12-31\n",
            "Holdout period: 2024-01-01 to 2024-12-31 23:00:00\n",
            "Training duration: ~10.0 years\n",
            "Holdout duration: 365 days\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "df = (\n",
        "    pd.read_csv(\"../../data/renewables/pv_with_weather_data.csv\",\n",
        "                comment=\"#\", parse_dates=[\"time\"])\n",
        "    .rename(columns={\"time\": \"ts\",\n",
        "                     \"electricity\": \"pv\",\n",
        "                     \"irradiance_direct\": \"dir_irr\",\n",
        "                     \"irradiance_diffuse\": \"dif_irr\",\n",
        "                     \"temperature\": \"temp\"})\n",
        "    .set_index(\"ts\")\n",
        "    .asfreq(STEP)\n",
        ")\n",
        "print(f\"Data span: {df.index.min()} – {df.index.max()} ({len(df):,} rows)\")\n",
        "print(f\"Training period: {df.index.min()} to 2023-12-31\")\n",
        "print(f\"Holdout period: 2024-01-01 to {df.index.max()}\")\n",
        "\n",
        "# Verify training vs holdout split\n",
        "train_years = (pd.to_datetime(\"2023-12-31\") - df.index.min()).days / 365.25\n",
        "holdout_days = (df.index.max() - pd.to_datetime(\"2024-01-01\")).days\n",
        "print(f\"Training duration: ~{train_years:.1f} years\")\n",
        "print(f\"Holdout duration: {holdout_days} days\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature matrix shape: (96408, 32)\n"
          ]
        }
      ],
      "source": [
        "def make_features(data, lags, include_time=True, include_weather=True):\n",
        "    \"\"\"Create feature matrix with lags, time, and weather features\"\"\"\n",
        "    X = pd.DataFrame(index=data.index)\n",
        "    \n",
        "    # Lag features\n",
        "    for lag in lags:\n",
        "        X[f\"lag_{lag}\"] = data[\"pv\"].shift(lag)\n",
        "\n",
        "    # Time features\n",
        "    if include_time:\n",
        "        hr = data.index.hour\n",
        "        dy = data.index.dayofyear\n",
        "        X[\"sin_hour\"] = np.sin(2 * np.pi * hr / 24)\n",
        "        X[\"cos_hour\"] = np.cos(2 * np.pi * hr / 24)\n",
        "        X[\"sin_doy\"]  = np.sin(2 * np.pi * dy / 365)\n",
        "        X[\"cos_doy\"]  = np.cos(2 * np.pi * dy / 365)\n",
        "    \n",
        "    # Weather features\n",
        "    if include_weather:\n",
        "        X[\"dir_irr\"] = data[\"dir_irr\"]\n",
        "        X[\"dif_irr\"] = data[\"dif_irr\"]\n",
        "        X[\"temp\"]    = data[\"temp\"]\n",
        "\n",
        "    y = data[\"pv\"]\n",
        "    return X.join(y.rename(\"target\")).dropna()\n",
        "\n",
        "feats_all = make_features(df, LAGS)\n",
        "print(\"Feature matrix shape:\", feats_all.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Data Splitting & Scaling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 70,099 | Val: 17,525 | Hold-out: 8,784\n",
            "Training years: ~8.0 years\n",
            "Holdout days: 366 days\n"
          ]
        }
      ],
      "source": [
        "# Split data\n",
        "train_val_end = \"2023-12-31 23:00\"\n",
        "holdout_start = \"2024-01-01 00:00\"\n",
        "\n",
        "train_val = feats_all.loc[:train_val_end]\n",
        "hold_out  = feats_all.loc[holdout_start:]\n",
        "\n",
        "# Internal chronological 80/20 split for training/validation\n",
        "split_idx = int(len(train_val) * 0.8)\n",
        "train = train_val.iloc[:split_idx]\n",
        "ival  = train_val.iloc[split_idx:]\n",
        "\n",
        "def split_xy(frame):\n",
        "    \"\"\"Split features and target\"\"\"\n",
        "    X = frame.drop(columns=\"target\").values\n",
        "    y = frame[\"target\"].values\n",
        "    return X, y\n",
        "\n",
        "Xt_tr, yt_tr = split_xy(train)\n",
        "Xt_val, yt_val = split_xy(ival)\n",
        "Xt_hold, yt_hold = split_xy(hold_out)\n",
        "\n",
        "# Scalers fitted on training data only\n",
        "x_scaler = StandardScaler().fit(Xt_tr)\n",
        "y_scaler = StandardScaler().fit(yt_tr.reshape(-1, 1))\n",
        "\n",
        "def scale_x(X): return x_scaler.transform(X)\n",
        "def unscale_y(y): return y_scaler.inverse_transform(y.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Scale data\n",
        "Xt_tr_s, Xt_val_s = scale_x(Xt_tr), scale_x(Xt_val)\n",
        "Xt_hold_s = scale_x(Xt_hold)\n",
        "yt_tr_s = y_scaler.transform(yt_tr.reshape(-1, 1)).ravel()\n",
        "\n",
        "print(f\"Train: {len(train):,} | Val: {len(ival):,} | Hold-out: {len(hold_out):,}\")\n",
        "print(f\"Training years: ~{len(train) / (365 * 24):.1f} years\")\n",
        "print(f\"Holdout days: {len(hold_out) / 24:.0f} days\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Function 1: Enhanced Hyperparameter Search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_parameter_search(param_ranges=None, max_trials=30, verbose=True):\n",
        "    \"\"\"\n",
        "    Run hyperparameter search with configurable parameter ranges.\n",
        "    \n",
        "    Args:\n",
        "        param_ranges (dict): Custom parameter ranges. If None, uses default ranges.\n",
        "        max_trials (int): Maximum number of trials for hyperparameter search\n",
        "        verbose (bool): Whether to print detailed results\n",
        "    \n",
        "    Returns:\n",
        "        dict: Best hyperparameters found\n",
        "    \"\"\"\n",
        "    \n",
        "    # Default parameter ranges\n",
        "    default_ranges = {\n",
        "        'n_layers': {'min_value': 1, 'max_value': 4},\n",
        "        'units': {'values': [32, 64, 128, 256]},\n",
        "        'dropout': {'min_value': 0.0, 'max_value': 0.5, 'step': 0.1},\n",
        "        'l2': {'min_value': 1e-6, 'max_value': 1e-2, 'sampling': 'log'},\n",
        "        'lr': {'values': [1e-4, 3e-4, 1e-3, 3e-3]}\n",
        "    }\n",
        "    \n",
        "    if param_ranges is None:\n",
        "        param_ranges = default_ranges\n",
        "    \n",
        "    def hp_model_enhanced(hp):\n",
        "        n_feat = Xt_tr_s.shape[1]\n",
        "        \n",
        "        # Define hyperparameters based on provided ranges\n",
        "        n_layers = hp.Int(\"n_layers\", **param_ranges['n_layers'])\n",
        "        units = hp.Choice(\"units\", param_ranges['units']['values'])\n",
        "        drop = hp.Float(\"dropout\", **{k:v for k,v in param_ranges['dropout'].items() if k != 'step'}, step=param_ranges['dropout'].get('step', 0.1))\n",
        "        l2_reg = hp.Float(\"l2\", **param_ranges['l2'])\n",
        "        lr = hp.Choice(\"lr\", param_ranges['lr']['values'])\n",
        "        \n",
        "        # Build model\n",
        "        m = models.Sequential()\n",
        "        m.add(layers.Input(shape=(n_feat,)))\n",
        "        \n",
        "        for _ in range(n_layers):\n",
        "            m.add(layers.Dense(units, activation=\"relu\",\n",
        "                               kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "            m.add(layers.Dropout(drop))\n",
        "        \n",
        "        m.add(layers.Dense(1))\n",
        "        m.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
        "                  loss=\"mse\", metrics=[\"mae\"])\n",
        "        return m\n",
        "    \n",
        "    # Create tuner\n",
        "    tuner = kt.RandomSearch(\n",
        "        hp_model_enhanced,\n",
        "        objective=\"val_loss\",\n",
        "        max_trials=max_trials,\n",
        "        executions_per_trial=EXEC_PER_TR,\n",
        "        directory=\"tuner_enhanced\",\n",
        "        project_name=\"pv_enhanced\"\n",
        "    )\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Search space:\")\n",
        "        tuner.search_space_summary()\n",
        "    \n",
        "    # Run search\n",
        "    es_cb = callbacks.EarlyStopping(patience=EARLY_STOP, restore_best_weights=True)\n",
        "    \n",
        "    print(f\"\\\\nStarting hyperparameter search with {max_trials} trials...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    tuner.search(\n",
        "        Xt_tr_s, yt_tr_s,\n",
        "        validation_data=(Xt_val_s, y_scaler.transform(yt_val.reshape(-1,1)).ravel()),\n",
        "        epochs=MAX_EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[es_cb],\n",
        "        verbose=0 if not verbose else 1\n",
        "    )\n",
        "    \n",
        "    search_time = time.time() - start_time\n",
        "    print(f\"Search completed in {search_time/60:.2f} minutes\")\n",
        "    \n",
        "    # Get best hyperparameters\n",
        "    best_hp = tuner.get_best_hyperparameters(1)[0]\n",
        "    best_params = best_hp.values\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"\\\\nBest hyperparameters found:\")\n",
        "        for param, value in best_params.items():\n",
        "            print(f\"  {param}: {value}\")\n",
        "        \n",
        "        # Show top 5 trials\n",
        "        print(\"\\\\nTop 5 trials:\")\n",
        "        tuner.results_summary(5)\n",
        "    \n",
        "    return best_params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Function 2: Random Date Evaluation & Error Propagation Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_random_dates_and_error_propagation(model_params, n_random_dates=10, \n",
        "                                                 analyze_error_propagation=True,\n",
        "                                                 verbose=True):\n",
        "    \"\"\"\n",
        "    Train model with best parameters and evaluate on random dates + error propagation.\n",
        "    \n",
        "    Args:\n",
        "        model_params (dict): Best hyperparameters to use\n",
        "        n_random_dates (int): Number of random dates to evaluate\n",
        "        analyze_error_propagation (bool): Whether to analyze error propagation\n",
        "        verbose (bool): Whether to print detailed results\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (model, daily_errors_df, random_dates_results, error_propagation_results)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Build model with best parameters\n",
        "    def create_best_model(params):\n",
        "        n_feat = Xt_tr_s.shape[1]\n",
        "        \n",
        "        m = models.Sequential()\n",
        "        m.add(layers.Input(shape=(n_feat,)))\n",
        "        \n",
        "        for _ in range(params['n_layers']):\n",
        "            m.add(layers.Dense(params['units'], activation=\"relu\",\n",
        "                               kernel_regularizer=regularizers.l2(params['l2'])))\n",
        "            m.add(layers.Dropout(params['dropout']))\n",
        "        \n",
        "        m.add(layers.Dense(1))\n",
        "        m.compile(optimizer=tf.keras.optimizers.Adam(params['lr']),\n",
        "                  loss=\"mse\", metrics=[\"mae\"])\n",
        "        return m\n",
        "    \n",
        "    # Train model on full training data (train + validation)\n",
        "    print(\"Training model with best parameters on full training set...\")\n",
        "    model = create_best_model(model_params)\n",
        "    \n",
        "    full_X = np.concatenate([Xt_tr_s, Xt_val_s])\n",
        "    full_y = np.concatenate([yt_tr_s, y_scaler.transform(yt_val.reshape(-1,1)).ravel()])\n",
        "    \n",
        "    es_cb = callbacks.EarlyStopping(patience=EARLY_STOP, restore_best_weights=True)\n",
        "    history = model.fit(\n",
        "        full_X, full_y,\n",
        "        epochs=MAX_EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[es_cb],\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    # Make predictions on holdout set\n",
        "    y_pred_s = model.predict(Xt_hold_s, verbose=0).flatten()\n",
        "    y_pred = unscale_y(y_pred_s)\n",
        "    \n",
        "    # Overall holdout performance\n",
        "    mae_overall = mean_absolute_error(yt_hold, y_pred)\n",
        "    rmse_overall = np.sqrt(mean_squared_error(yt_hold, y_pred))\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\\\nOverall 2024 Performance:\")\n",
        "        print(f\"  MAE: {mae_overall:.4f}\")\n",
        "        print(f\"  RMSE: {rmse_overall:.4f}\")\n",
        "    \n",
        "    # Create results dataframe\n",
        "    results_df = pd.DataFrame({\n",
        "        \"actual\": yt_hold,\n",
        "        \"predicted\": y_pred,\n",
        "        \"error\": yt_hold - y_pred,\n",
        "        \"abs_error\": np.abs(yt_hold - y_pred)\n",
        "    }, index=hold_out.index)\n",
        "    \n",
        "    return model, results_df, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_daily_errors(results_df, verbose=True):\n",
        "    \"\"\"\n",
        "    Analyze daily error patterns from results DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        results_df: DataFrame with actual, predicted, error, abs_error columns\n",
        "        verbose: Whether to print detailed results\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame: Daily error statistics\n",
        "    \"\"\"\n",
        "    daily_errors = []\n",
        "    unique_dates = results_df.index.normalize().unique()\n",
        "    \n",
        "    for date in unique_dates:\n",
        "        day_data = results_df[results_df.index.normalize() == date]\n",
        "        if len(day_data) > 0:\n",
        "            daily_errors.append({\n",
        "                'date': date.date(),\n",
        "                'mae': day_data['abs_error'].mean(),\n",
        "                'rmse': np.sqrt(day_data['error'].pow(2).mean()),\n",
        "                'max_error': day_data['abs_error'].max(),\n",
        "                'std_error': day_data['abs_error'].std(),\n",
        "                'mean_actual': day_data['actual'].mean(),\n",
        "                'mean_predicted': day_data['predicted'].mean(),\n",
        "                'day_of_year': date.dayofyear,\n",
        "                'month': date.month,\n",
        "                'weekday': date.weekday()\n",
        "            })\n",
        "    \n",
        "    daily_errors_df = pd.DataFrame(daily_errors)\n",
        "    daily_errors_df['date'] = pd.to_datetime(daily_errors_df['date'])\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\\\n=== DAILY ERROR ANALYSIS ===\")\n",
        "        print(f\"Total days analyzed: {len(daily_errors_df)}\")\n",
        "        print(f\"Average daily MAE: {daily_errors_df['mae'].mean():.4f}\")\n",
        "        print(f\"MAE std deviation: {daily_errors_df['mae'].std():.4f}\")\n",
        "        print(f\"Best day (lowest MAE): {daily_errors_df.loc[daily_errors_df['mae'].idxmin(), 'date'].strftime('%Y-%m-%d')} ({daily_errors_df['mae'].min():.4f})\")\n",
        "        print(f\"Worst day (highest MAE): {daily_errors_df.loc[daily_errors_df['mae'].idxmax(), 'date'].strftime('%Y-%m-%d')} ({daily_errors_df['mae'].max():.4f})\")\n",
        "    \n",
        "    return daily_errors_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_error_propagation(daily_errors_df, verbose=True):\n",
        "    \"\"\"\n",
        "    Analyze error propagation patterns throughout the year.\n",
        "    \n",
        "    Args:\n",
        "        daily_errors_df: DataFrame with daily error statistics\n",
        "        verbose: Whether to print detailed results\n",
        "    \n",
        "    Returns:\n",
        "        dict: Error propagation analysis results\n",
        "    \"\"\"\n",
        "    # Weekly error analysis\n",
        "    daily_errors_df['week'] = daily_errors_df['date'].dt.isocalendar().week\n",
        "    weekly_errors = daily_errors_df.groupby('week').agg({\n",
        "        'mae': ['mean', 'std', 'min', 'max'],\n",
        "        'rmse': ['mean', 'std', 'min', 'max'],\n",
        "        'date': 'count'\n",
        "    }).round(4)\n",
        "    \n",
        "    # Monthly error analysis\n",
        "    monthly_errors = daily_errors_df.groupby('month').agg({\n",
        "        'mae': ['mean', 'std', 'min', 'max'],\n",
        "        'rmse': ['mean', 'std', 'min', 'max'],\n",
        "        'date': 'count'\n",
        "    }).round(4)\n",
        "    \n",
        "    # Seasonal trends\n",
        "    daily_errors_df['season'] = daily_errors_df['month'].map({\n",
        "        12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
        "        3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
        "        6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
        "        9: 'Fall', 10: 'Fall', 11: 'Fall'\n",
        "    })\n",
        "    \n",
        "    seasonal_errors = daily_errors_df.groupby('season').agg({\n",
        "        'mae': ['mean', 'std', 'min', 'max'],\n",
        "        'rmse': ['mean', 'std', 'min', 'max'],\n",
        "        'date': 'count'\n",
        "    }).round(4)\n",
        "    \n",
        "    # Trend analysis\n",
        "    mae_trend = np.corrcoef(daily_errors_df['day_of_year'], daily_errors_df['mae'])[0,1]\n",
        "    rmse_trend = np.corrcoef(daily_errors_df['day_of_year'], daily_errors_df['rmse'])[0,1]\n",
        "    \n",
        "    # Weekday analysis\n",
        "    weekday_errors = daily_errors_df.groupby('weekday').agg({\n",
        "        'mae': ['mean', 'std'],\n",
        "        'rmse': ['mean', 'std']\n",
        "    }).round(4)\n",
        "    \n",
        "    error_propagation = {\n",
        "        'weekly': weekly_errors,\n",
        "        'monthly': monthly_errors,\n",
        "        'seasonal': seasonal_errors,\n",
        "        'weekday': weekday_errors,\n",
        "        'trend_analysis': {\n",
        "            'mae_trend': mae_trend,\n",
        "            'rmse_trend': rmse_trend\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\\\n=== ERROR PROPAGATION ANALYSIS ===\")\n",
        "        print(f\"MAE trend correlation with day of year: {mae_trend:.4f}\")\n",
        "        print(f\"RMSE trend correlation with day of year: {rmse_trend:.4f}\")\n",
        "        print(f\"\\\\nSeasonal MAE averages:\")\n",
        "        for season in ['Winter', 'Spring', 'Summer', 'Fall']:\n",
        "            if season in seasonal_errors.index:\n",
        "                mae_mean = seasonal_errors.loc[season, ('mae', 'mean')]\n",
        "                print(f\"  {season}: {mae_mean:.4f}\")\n",
        "        \n",
        "        print(f\"\\\\nMonthly MAE averages:\")\n",
        "        for month in range(1, 13):\n",
        "            if month in monthly_errors.index:\n",
        "                mae_mean = monthly_errors.loc[month, ('mae', 'mean')]\n",
        "                print(f\"  Month {month}: {mae_mean:.4f}\")\n",
        "    \n",
        "    return error_propagation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_random_dates(results_df, daily_errors_df, n_random_dates=10, verbose=True):\n",
        "    \"\"\"\n",
        "    Evaluate model performance on random dates.\n",
        "    \n",
        "    Args:\n",
        "        results_df: DataFrame with hourly predictions\n",
        "        daily_errors_df: DataFrame with daily error statistics\n",
        "        n_random_dates: Number of random dates to evaluate\n",
        "        verbose: Whether to print detailed results\n",
        "    \n",
        "    Returns:\n",
        "        list: Random dates evaluation results\n",
        "    \"\"\"\n",
        "    available_dates = daily_errors_df['date'].tolist()\n",
        "    random_dates = random.sample(available_dates, min(n_random_dates, len(available_dates)))\n",
        "    \n",
        "    random_results = []\n",
        "    for date in random_dates:\n",
        "        day_data = results_df[results_df.index.normalize() == date.normalize()]\n",
        "        day_errors = daily_errors_df[daily_errors_df['date'] == date].iloc[0]\n",
        "        \n",
        "        random_results.append({\n",
        "            'date': date.date(),\n",
        "            'mae': day_errors['mae'],\n",
        "            'rmse': day_errors['rmse'],\n",
        "            'max_error': day_errors['max_error'],\n",
        "            'hourly_data': day_data[['actual', 'predicted', 'abs_error']].copy()\n",
        "        })\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\\\n=== RANDOM {n_random_dates} DATES EVALUATION ===\")\n",
        "        for result in random_results:\n",
        "            print(f\"  {result['date']}: MAE={result['mae']:.4f}, RMSE={result['rmse']:.4f}, Max_Error={result['max_error']:.4f}\")\n",
        "    \n",
        "    return random_results\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Visualization Functions\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vt2-optimization-datamodels-NPqtCblQ-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
